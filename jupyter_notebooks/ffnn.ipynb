{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import numpy.typing as npt\n",
    "import csv\n",
    "from collections import Counter\n",
    "\n",
    "from typing import Tuple,List,Set\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filename: str) -> Tuple[List[str], List[str], Set[str]]:\n",
    "    intent = []\n",
    "    unique_intent = []\n",
    "    sentences = []\n",
    "    with open(filename, \"r\", encoding=\"latin1\") as f:\n",
    "        data = csv.reader(f, delimiter=\",\")\n",
    "        for row in data:\n",
    "            sentences.append(row[0])\n",
    "            intent.append(row[1])\n",
    "    unique_intent = set(intent)\n",
    "    return sentences, intent, unique_intent\n",
    "\n",
    "# sentences, intent, unique_intent = load_dataset(\"../data/dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bag_of_words_matrix(sentences: List[str],COUNT_THRESHOLD: int) -> npt.ArrayLike:\n",
    "    \"\"\"\n",
    "    Convert the dataset into V x M matrix.\n",
    "    \"\"\"\n",
    "    token_list = [sentence.strip().split(\" \") for sentence in sentences]\n",
    "    token_list = [token.strip().lower() for token in sum(token_list,[])]\n",
    "\n",
    "    word_dict = Counter(token_list)\n",
    "    COUNT_THRESHOLD = 2\n",
    "    word_dict_t = {\"<UNK>\":0}\n",
    "\n",
    "    for word, count in word_dict.items():\n",
    "        if count <= COUNT_THRESHOLD:\n",
    "            word_dict_t[\"<UNK>\"] += count\n",
    "        else:\n",
    "            word_dict_t[word] = count\n",
    "            \n",
    "    vocab = list(word_dict_t.keys())\n",
    "\n",
    "    word_matrix = np.zeros(shape=(len(sentences),len(vocab)))\n",
    "\n",
    "    for sent_index,sentence in enumerate(sentences):\n",
    "        sent = [token if token in vocab else \"<UNK>\" for token in sentence.strip().split(\" \")]\n",
    "        sent_count_dict = Counter(sent)\n",
    "        for vocab_index,vocab_word in enumerate(vocab):\n",
    "            if vocab_word in sent_count_dict.keys():\n",
    "                word_matrix[sent_index,vocab_index] = sent_count_dict[vocab_word]            \n",
    "    word_matrix = word_matrix.T\n",
    "    print(word_matrix.shape)\n",
    "    \n",
    "    return word_matrix\n",
    "\n",
    "def labels_matrix(data: Tuple[List[str], Set[str]]) -> npt.ArrayLike:\n",
    "    \"\"\"\n",
    "    Convert the dataset into K x M matrix.\n",
    "    \"\"\"\n",
    "    intent = data[0]\n",
    "    intent_vocab = data[1]\n",
    "\n",
    "    label_matrix = np.zeros(shape=(len(intent),len(intent_vocab)))\n",
    "    \n",
    "    for index, label in enumerate(intent):\n",
    "        for int_index, intent in enumerate(intent_vocab):\n",
    "            if label == intent:\n",
    "                label_matrix[index,int_index] = 1\n",
    "    \n",
    "    label_matrix = label_matrix.T\n",
    "    print(label_matrix.shape)\n",
    "    \n",
    "    return label_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(287, 1000)\n",
      "(7, 1000)\n",
      "(287, 1000)\n"
     ]
    }
   ],
   "source": [
    "X = bag_of_words_matrix(sentences=sentences,COUNT_THRESHOLD=2)\n",
    "# bias = np.ones(shape=X.shape[1]).reshape(1,X.shape[1])\n",
    "# X = np.append(bias,X,axis=0)\n",
    "y = labels_matrix(data=(intent,unique_intent))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "internal_weights = np.random.uniform(low = -1.0,\n",
    "                                    high = 1.0,\n",
    "                                    size = (288,150),\n",
    "                                    )\n",
    "output_weights = np.random.uniform(low = -1,\n",
    "                                        high = 1,\n",
    "                                        size = (150,7)\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z = np.matmul(weights.T,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE = 10\n",
    "# _current = 0 \n",
    "# for batch_start, batch_stop in zip(np.arange(0,X.shape[1],BATCH_SIZE),np.arange(BATCH_SIZE,X.shape[1],BATCH_SIZE)):\n",
    "#     # print(batch_start,batch_stop)\n",
    "#     X_data = X[:,batch_start:batch_stop]\n",
    "#     # _current = batch\n",
    "#     # print(X_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z: npt.ArrayLike) -> npt.ArrayLike:\n",
    "    \"\"\"\n",
    "    Rectified Linear Unit function.\n",
    "    \"\"\"\n",
    "    return np.array([np.max([0,i]) for i in z])\n",
    "\n",
    "def softmax(z: npt.ArrayLike) -> npt.ArrayLike:\n",
    "    \"\"\"\n",
    "    Softmax function.\n",
    "    \"\"\"\n",
    "    assert type(z) == np.ndarray\n",
    "    return np.exp(z.T) / np.sum(np.exp(z))\n",
    "\n",
    "def relu_prime(z: npt.ArrayLike) -> npt.ArrayLike:\n",
    "    \"\"\"\n",
    "    First derivative of ReLU function.\n",
    "    \"\"\"\n",
    "    return np.array([1 if i > 0 else 0 for i in z])\n",
    "\n",
    "# def cross_entropy_loss(y_true, y_pred):\n",
    "#     \"\"\"\n",
    "#     Cross-Entropy Loss\n",
    "#     \"\"\"\n",
    "#     epsilon = 1e-15\n",
    "#     y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "#     return -np.sum(y_true * np.log(y_pred)) / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = np.array([relu(i) for i in z])\n",
    "# a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_weights = np.random.uniform(-1,1,(a.shape[0],y.shape[0]))\n",
    "# output_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z_out = np.matmul(output_weights.T,a)\n",
    "# z_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_out = np.array([softmax(node) for node in z_out.T]).T\n",
    "# a_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X: npt.ArrayLike) -> npt.ArrayLike:\n",
    "    \"\"\"\n",
    "    Forward pass with X as input matrix, returning the model prediction\n",
    "    Y_hat.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        bias = np.ones(shape=X.shape[1]).reshape(1,X.shape[1])\n",
    "        X = np.append(bias,X,axis=0)\n",
    "        # print(f\"Shape of Input Matrix : {X.shape}\")\n",
    "        z_internal: npt.ArrayLike = np.matmul(internal_weights.T,X)\n",
    "        # print(f\"Shape of internal weights : {z_internal.shape}\")\n",
    "        a_internal: npt.ArrayLike = np.array([relu(node) for node in z_internal])\n",
    "        # print(a_internal.shape)\n",
    "        z_output: npt.ArrayLike = np.matmul(output_weights.T,a_internal)\n",
    "        # print(f\"Shape of output weights : {z_output.shape}\")\n",
    "        a_output: npt.ArrayLike = np.array([softmax(node) for node in z_output.T]).T\n",
    "        # print(a_output.shape)\n",
    "        \n",
    "    except IndexError:\n",
    "        bias = [1]\n",
    "        X = np.append(bias,X,axis=0)\n",
    "        # print(f\"Shape of Input Matrix : {X.shape}\")\n",
    "        z_internal: npt.ArrayLike = np.matmul(internal_weights.T,X)\n",
    "        # print(f\"Shape of internal weights : {z_internal.shape}\")\n",
    "        a_internal: npt.ArrayLike = relu(z_internal)\n",
    "        # print(a_internal.shape)\n",
    "        z_output: npt.ArrayLike = np.matmul(output_weights.T,a_internal)\n",
    "        # print(f\"Shape of output weights : {z_output.shape}\")\n",
    "        a_output: npt.ArrayLike = softmax(z_output)\n",
    "        # print(a_output.shape)\n",
    "    \n",
    "    return X, z_internal, a_internal, z_output, a_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_w_bias, z_internal, a_internal, z_output, a_output = forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Input Matrix : (288, 1000)\n",
      "Shape of internal weights : (150, 1000)\n",
      "Shape of output weights : (7, 1000)\n"
     ]
    }
   ],
   "source": [
    "X_w_bias, z_internal, a_internal, z_output, a_output = forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_index = np.argmax(a_output,axis=0)\n",
    "# prediction = np.zeros(shape=a_output.shape)\n",
    "# for i in range(len(max_index)):\n",
    "#     prediction[max_index[i],i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     prediction = np.zeros(shape=(a_output.shape[0],a_output.shape[1]))\n",
    "#     max_index = np.argmax(a_output,axis=0)\n",
    "#     for i in range(len(max_index)):\n",
    "#         prediction[max_index[i],i] = 1\n",
    "# except IndexError:\n",
    "#     prediction = np.zeros(shape=a_output.shape[0])\n",
    "#     prediction[a_output.argmax()] = 1\n",
    "#     prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X: npt.ArrayLike) -> npt.ArrayLike:\n",
    "        \"\"\"\n",
    "        Create a prediction matrix with `self.forward()`\n",
    "        \"\"\"\n",
    "        _, _, _, _, a_output = forward(X)\n",
    "        \n",
    "        try:\n",
    "            prediction = np.zeros(shape=(a_output.shape[0],a_output.shape[1]))\n",
    "            max_index = np.argmax(a_output,axis=0)\n",
    "            for i in range(len(max_index)):\n",
    "                prediction[max_index[i],i] = 1\n",
    "        except IndexError:\n",
    "            prediction = np.zeros(shape=a_output.shape[0])\n",
    "            prediction[a_output.argmax()] = 1\n",
    "        \n",
    "        # print(f\"Shape of prediction matrix : {prediction.shape}\")\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Input Matrix : (288, 10)\n",
      "Shape of internal weights : (150, 10)\n",
      "Shape of output weights : (7, 10)\n",
      "Shape of prediction matrix : (7, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 1., 1., 0., 1., 0., 1., 1., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(X[:,:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 288\n",
    "hidden_size = 150  # Number of neurons in the hidden layer\n",
    "output_size = 7  # Number of classes\n",
    "learning_rate = 0.005\n",
    "\n",
    "# batch_size = 1000\n",
    "# cross_entropy_delta = np.subtract(a_output, y)  # Derivative of softmax with cross-entropy loss\n",
    "# dW2 = np.matmul(a_internal, cross_entropy_delta.T)\n",
    "# dW2.shape\n",
    "\n",
    "# output_weights.shape\n",
    "\n",
    "\n",
    "# dz1 = np.matmul(output_weights,cross_entropy_delta) * np.array([relu_prime(z) for z in z_internal.T]).T\n",
    "# dW1 = np.dot(X_w_bias,dz1.T) / batch_size\n",
    "\n",
    "# # Update weights and biases\n",
    "# output_weights -= learning_rate * dW2\n",
    "# internal_weights -= learning_rate * dW1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0222381700476624\n",
      "-0.0008302572176805077\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(output_weights))\n",
    "print(np.mean(internal_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(\n",
    "        X: npt.ArrayLike, \n",
    "        Y: npt.ArrayLike,\n",
    "        internal_weights,\n",
    "        output_weights,\n",
    "        learning_rate,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Backpropagation algorithm.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            batch_size = X.shape[1]\n",
    "            X_w_bias, z_internal, a_internal, z_output, a_output = forward(X)\n",
    "            # prediction_matrix = predict(X)\n",
    "            \n",
    "            loss = cross_entropy_loss(Y, a_output)\n",
    "            cross_entropy_delta = np.subtract(a_output, Y)  # Derivative of softmax with cross-entropy loss\n",
    "            \n",
    "            grad_output_layer = np.matmul(a_internal.T, cross_entropy_delta) / batch_size\n",
    "            grad_hidden_input = np.matmul(output_weights,cross_entropy_delta) * np.array([relu_prime(z) for z in z_internal.T]).T\n",
    "            grad_hidden_layer = np.dot(X_w_bias,grad_hidden_input.T) / batch_size\n",
    "            \n",
    "            output_weights -= learning_rate * grad_output_layer\n",
    "            internal_weights -= learning_rate * grad_hidden_layer\n",
    "        \n",
    "        except IndexError:\n",
    "            X_w_bias, z_internal, a_internal, z_output, a_output = forward(X)\n",
    "            # prediction_matrix = predict(X)\n",
    "            \n",
    "            loss = cross_entropy_loss(Y, a_output)\n",
    "            cross_entropy_delta = np.subtract(a_output, Y)  # Derivative of softmax with cross-entropy loss\n",
    "            \n",
    "            grad_output_layer = np.matmul(a_internal.reshape(hidden_size,1), cross_entropy_delta.reshape(output_size,1).T)\n",
    "            grad_hidden_input = np.matmul(output_weights,cross_entropy_delta) * relu_prime(z_internal)\n",
    "            grad_hidden_layer = np.dot(X_w_bias.reshape(input_size,1),grad_hidden_input.reshape(hidden_size,1).T)  \n",
    "                      \n",
    "            output_weights -= learning_rate * grad_output_layer\n",
    "            internal_weights -= learning_rate * grad_hidden_layer\n",
    "        \n",
    "        return loss, internal_weights, output_weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(42)\n",
    "# internal_weights = np.random.uniform(low = -1.0,\n",
    "#                                     high = 1.0,\n",
    "#                                     size = (288,150),\n",
    "#                                     )\n",
    "# output_weights = np.random.uniform(low = -1,\n",
    "#                                         high = 1,\n",
    "#                                         size = (150,7)\n",
    "#                                         )\n",
    "# for i in range(1000):\n",
    "#     loss, internal_weights, output_weights = backward(X[:,i],y[:,i],internal_weights,output_weights,0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict(X[:,32:42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y[:,32:42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# internal_weights = np.random.uniform(low = -1.0,\n",
    "#                                     high = 1.0,\n",
    "#                                     size = (288,150),\n",
    "#                                     )\n",
    "# output_weights = np.random.uniform(low = -1,\n",
    "#                                         high = 1,\n",
    "#                                         size = (150,7)\n",
    "#                                         )\n",
    "# X_w_bias, z_internal, a_internal, z_output, a_output = forward(X[:,0])\n",
    "# # prediction_matrix = predict(X)\n",
    "\n",
    "# loss = cross_entropy_loss(y[:,0], a_output)\n",
    "# cross_entropy_delta = np.subtract(a_output, y[:,0])  # Derivative of softmax with cross-entropy loss\n",
    "# dW2 = np.matmul(a_internal.reshape(hidden_size,1), cross_entropy_delta.reshape(output_size,1).T)\n",
    "\n",
    "# np.matmul(output_weights,cross_entropy_delta) * relu_prime(z_internal)\n",
    "\n",
    "# dz1 = np.matmul(output_weights,cross_entropy_delta) * relu_prime(z_internal)\n",
    "# dW1 = np.dot(X_w_bias.reshape(input_size,1),dz1.reshape(hidden_size,1).T)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_size: int,\n",
    "        hidden_size: int, \n",
    "        num_classes: int,\n",
    "        random_seed: int = 42,\n",
    "        learning_rate: float = 0.005,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize neural network's weights and biases.\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = num_classes\n",
    "        self.num_classes = num_classes\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        np.random.seed(random_seed)\n",
    "        self.internal_weights = np.random.uniform(low = -1.0,\n",
    "                                         high = 1.0,\n",
    "                                         size = (input_size,hidden_size),\n",
    "                                         )\n",
    "        self.output_weights = np.random.uniform(low = -1,\n",
    "                                                high = 1,\n",
    "                                                size = (hidden_size,num_classes)\n",
    "                                                )\n",
    "        \n",
    "    def forward(self,\n",
    "                X: npt.ArrayLike\n",
    "                )-> Tuple[npt.ArrayLike, npt.ArrayLike, npt.ArrayLike, npt.ArrayLike, npt.ArrayLike]:\n",
    "        \"\"\"\n",
    "        Forward pass with X as input matrix, returning the self prediction\n",
    "        Y_hat.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            bias = np.ones(shape=X.shape[1]).reshape(1,X.shape[1])\n",
    "            X = np.append(bias,X,axis=0)\n",
    "            # print(f\"Shape of Input Matrix : {X.shape}\")\n",
    "            z_internal: npt.ArrayLike = np.matmul(self.internal_weights.T,X)\n",
    "            # print(f\"Shape of internal weights : {z_internal.shape}\")\n",
    "            a_internal: npt.ArrayLike = np.array([relu(node) for node in z_internal])\n",
    "            # print(a_internal.shape)\n",
    "            z_output: npt.ArrayLike = np.matmul(self.output_weights.T,a_internal)\n",
    "            # print(f\"Shape of output weights : {z_output.shape}\")\n",
    "            a_output: npt.ArrayLike = np.array([softmax(node) for node in z_output.T]).T\n",
    "            # print(a_output.shape)\n",
    "            \n",
    "        except IndexError:\n",
    "            bias = [1]\n",
    "            X = np.append(bias,X,axis=0)\n",
    "            # print(f\"Shape of Input Matrix : {X.shape}\")\n",
    "            z_internal: npt.ArrayLike = np.matmul(self.internal_weights.T,X)\n",
    "            # print(f\"Shape of internal weights : {z_internal.shape}\")\n",
    "            a_internal: npt.ArrayLike = relu(z_internal)\n",
    "            # print(a_internal.shape)\n",
    "            z_output: npt.ArrayLike = np.matmul(self.output_weights.T,a_internal)\n",
    "            # print(f\"Shape of output weights : {z_output.shape}\")\n",
    "            a_output: npt.ArrayLike = softmax(z_output.reshape(z_output.shape[0],1))\n",
    "            # print(a_output.shape)\n",
    "        \n",
    "        return X, z_internal, a_internal, z_output, a_output\n",
    "\n",
    "    def predict(self, X: npt.ArrayLike) -> npt.ArrayLike:\n",
    "        \"\"\"\n",
    "        Create a prediction matrix with `self.forward()`\n",
    "        \"\"\"\n",
    "        try:\n",
    "            prediction = []\n",
    "            for i in range(X.shape[1]):\n",
    "                _, _, _, _, a_output = self.forward(X[:,i])\n",
    "                prediction_int = np.zeros(shape=a_output.shape[1])\n",
    "                prediction_int[a_output.argmax()] = 1\n",
    "                prediction.append(list(prediction_int))\n",
    "            prediction = np.array(prediction).T\n",
    "        except IndexError:\n",
    "            _, _, _, _, a_output = self.forward(X)\n",
    "            prediction = np.zeros(shape=a_output.shape[0])\n",
    "            prediction[a_output.argmax()] = 1\n",
    "        \n",
    "        # print(prediction.shape)\n",
    "        return prediction\n",
    "\n",
    "    def backward(\n",
    "        self, \n",
    "        X: npt.ArrayLike, \n",
    "        Y: npt.ArrayLike\n",
    "    ) -> Tuple[npt.ArrayLike, npt.ArrayLike, npt.ArrayLike, npt.ArrayLike]:\n",
    "        \"\"\"\n",
    "        Backpropagation algorithm.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            batch_size = X.shape[1]\n",
    "            X_bias, z_internal, a_internal, _, a_output = self.forward(X)\n",
    "\n",
    "            cross_entropy_delta = np.subtract(a_output, Y)  # Derivative of softmax with cross-entropy loss\n",
    "            grad_output_layer = np.matmul(a_internal.T, cross_entropy_delta) / batch_size\n",
    "            grad_hidden_input = np.matmul(self.output_weights,cross_entropy_delta) * np.array([relu_prime(z) for z in z_internal.T]).T\n",
    "            grad_hidden_layer = np.dot(X_bias,grad_hidden_input.T) / batch_size\n",
    "        \n",
    "        except IndexError:\n",
    "            X_bias, z_internal, a_internal, _, a_output = self.forward(X)\n",
    "            Y = Y.reshape(self.output_size,1)\n",
    "            z_internal = z_internal.reshape(self.hidden_size,1)\n",
    "            a_internal = a_internal.reshape(self.hidden_size,1)\n",
    "            \n",
    "            cross_entropy_delta = np.subtract(a_output, Y)  # Derivative of softmax with cross-entropy loss\n",
    "            grad_output_layer = np.matmul(a_internal, cross_entropy_delta.T)\n",
    "            grad_hidden_input = np.matmul(self.output_weights,cross_entropy_delta) * relu_prime(z_internal).reshape(self.hidden_size,1)\n",
    "            grad_hidden_layer = np.dot(X_bias.reshape(self.input_size,1),grad_hidden_input.reshape(self.hidden_size,1).T)  \n",
    "            \n",
    "        return grad_hidden_layer, grad_output_layer\n",
    "    \n",
    "    def batch_train(self, X, Y, train_flag=False):\n",
    "        prediction = self.predict(X)\n",
    "        hits = np.sum([np.array_equal(prediction[:,i],Y[:,i]) for i in range(X.shape[1])])\n",
    "        print(f\"Accuracy of self : {hits*100/X.shape[1]} %\")\n",
    "        \n",
    "        if train_flag:\n",
    "            loss_list = []\n",
    "            accuracy_list =[]\n",
    "            for i in range(X.shape[1]):\n",
    "                grad_hidden_layer, grad_output_layer = self.backward(X[:,i],Y[:,i])\n",
    "                self.internal_weights -= self.learning_rate * grad_hidden_layer\n",
    "                self.output_weights -= self.learning_rate * grad_output_layer\n",
    "                # prediction = self.predict(X)\n",
    "                # loss_list.append(compute_loss(prediction,Y))\n",
    "                if i%25 == 0:\n",
    "                    print(f\"Inputs processed : {i}\")\n",
    "                    prediction = self.predict(X)\n",
    "                    hits = np.sum([np.array_equal(prediction[:,i],Y[:,i]) for i in range(X.shape[1])])\n",
    "                    loss_list.append(compute_loss(prediction,Y))\n",
    "                    accuracy_list.append(hits*100/1000)\n",
    "                    print(f\"Current loss : {compute_loss(prediction,Y)} \\nCurrent Accuracy : {hits*100/X.shape[1]} %\")\n",
    "                        \n",
    "            prediction = self.predict(X)\n",
    "            hits = np.sum([np.array_equal(prediction[:,i],Y[:,i]) for i in range(X.shape[1])])\n",
    "            print(f\"Accuracy of self : {hits*100/X.shape[1]} %\")\n",
    "            \n",
    "            plt.style.use(\"seaborn\")\n",
    "            fig,ax = plt.subplots(nrows=2,ncols=1,figsize=(15,10),sharex=True)\n",
    "            ax[0].plot(range(1,X.shape[1]+1,25),loss_list,\"k-\",linewidth=2)\n",
    "            ax[1].plot(range(1,X.shape[1]+1,25),accuracy_list,\"r-\",linewidth=2)\n",
    "            plt.xlabel(\"Iteration\",fontdict={\"size\":14,\"weight\":\"bold\"})\n",
    "            ax[0].set_ylabel(\"Cost Function\",fontdict={\"size\":14,\"weight\":\"bold\"})\n",
    "            ax[1].set_ylabel(\"Accuracy\",fontdict={\"size\":14,\"weight\":\"bold\"})\n",
    "            plt.xticks(range(X.shape[1]+1)[::100])\n",
    "            fig.tight_layout()\n",
    "            fig.savefig(\"./images/Cost_Function_Training.jpeg\",dpi=300)\n",
    "            print(\"Image saved at : ./images/Cost_Function_Training.jpeg\")\n",
    "\n",
    "\n",
    "def compute_loss(pred: npt.ArrayLike, truth: npt.ArrayLike) -> float:\n",
    "    \"\"\"\n",
    "    Compute the cross entropy loss.\n",
    "    \"\"\"\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(pred, epsilon, 1 - epsilon)\n",
    "    \n",
    "    return -np.mean(np.sum(truth * np.log(y_pred),axis=0)) / len(truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(input_size=X.shape[0]+1,\n",
    "                          hidden_size=150,\n",
    "                          num_classes=y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model : 18.2 %\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(X)\n",
    "hits = np.sum([np.array_equal(prediction[:,i],y[:,i]) for i in range(1000)])\n",
    "print(f\"Accuracy of model : {hits*100/1000} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs processed : 0\n",
      "Current loss : 4.07557561459946 Current Accuracy : 17.4 %\n",
      "Inputs processed : 25\n",
      "Current loss : 4.055839170945225 Current Accuracy : 17.8 %\n",
      "Inputs processed : 50\n",
      "Current loss : 3.6759126306012075 Current Accuracy : 25.5 %\n",
      "Inputs processed : 75\n",
      "Current loss : 3.8683429562299954 Current Accuracy : 21.6 %\n",
      "Inputs processed : 100\n",
      "Current loss : 3.4933505267995373 Current Accuracy : 29.2 %\n",
      "Inputs processed : 125\n",
      "Current loss : 3.547625746848682 Current Accuracy : 28.1 %\n",
      "Inputs processed : 150\n",
      "Current loss : 3.172633317418224 Current Accuracy : 35.7 %\n",
      "Inputs processed : 175\n",
      "Current loss : 3.167699206504665 Current Accuracy : 35.8 %\n",
      "Inputs processed : 200\n",
      "Current loss : 3.1874356501589 Current Accuracy : 35.4 %\n",
      "Inputs processed : 225\n",
      "Current loss : 2.7482997788521786 Current Accuracy : 44.3 %\n",
      "Inputs processed : 250\n",
      "Current loss : 2.6249470060132123 Current Accuracy : 46.8 %\n",
      "Inputs processed : 275\n",
      "Current loss : 2.565737675050508 Current Accuracy : 48.0 %\n",
      "Inputs processed : 300\n",
      "Current loss : 2.3881096821623955 Current Accuracy : 51.6 %\n",
      "Inputs processed : 325\n",
      "Current loss : 2.3091639075454573 Current Accuracy : 53.2 %\n",
      "Inputs processed : 350\n",
      "Current loss : 2.180877023792932 Current Accuracy : 55.8 %\n",
      "Inputs processed : 375\n",
      "Current loss : 2.1956793565336077 Current Accuracy : 55.5 %\n",
      "Inputs processed : 400\n",
      "Current loss : 2.1611405801386967 Current Accuracy : 56.2 %\n",
      "Inputs processed : 425\n",
      "Current loss : 2.249954576582753 Current Accuracy : 54.4 %\n",
      "Inputs processed : 450\n",
      "Current loss : 2.166074691052256 Current Accuracy : 56.1 %\n",
      "Inputs processed : 475\n",
      "Current loss : 2.00324903090482 Current Accuracy : 59.4 %\n",
      "Inputs processed : 500\n",
      "Current loss : 1.8404233707573834 Current Accuracy : 62.7 %\n",
      "Inputs processed : 525\n",
      "Current loss : 1.865093925325177 Current Accuracy : 62.2 %\n",
      "Inputs processed : 550\n",
      "Current loss : 1.9884466981641438 Current Accuracy : 59.7 %\n",
      "Inputs processed : 575\n",
      "Current loss : 1.894698590806529 Current Accuracy : 61.6 %\n",
      "Inputs processed : 600\n",
      "Current loss : 1.8601598144116185 Current Accuracy : 62.3 %\n",
      "Inputs processed : 625\n",
      "Current loss : 1.7516093743133274 Current Accuracy : 64.5 %\n",
      "Inputs processed : 650\n",
      "Current loss : 1.638124823301478 Current Accuracy : 66.8 %\n",
      "Inputs processed : 675\n",
      "Current loss : 1.6035860469065675 Current Accuracy : 67.5 %\n",
      "Inputs processed : 700\n",
      "Current loss : 1.5147720504625115 Current Accuracy : 69.3 %\n",
      "Inputs processed : 725\n",
      "Current loss : 1.3963533885371036 Current Accuracy : 71.7 %\n",
      "Inputs processed : 750\n",
      "Current loss : 1.3618146121421926 Current Accuracy : 72.4 %\n",
      "Inputs processed : 775\n",
      "Current loss : 1.322341724833723 Current Accuracy : 73.2 %\n",
      "Inputs processed : 800\n",
      "Current loss : 1.3371440575743991 Current Accuracy : 72.9 %\n",
      "Inputs processed : 825\n",
      "Current loss : 1.3470122794015167 Current Accuracy : 72.7 %\n",
      "Inputs processed : 850\n",
      "Current loss : 1.292737059352371 Current Accuracy : 73.8 %\n",
      "Inputs processed : 875\n",
      "Current loss : 1.2779347266116954 Current Accuracy : 74.1 %\n",
      "Inputs processed : 900\n",
      "Current loss : 1.2878029484388125 Current Accuracy : 73.9 %\n",
      "Inputs processed : 925\n",
      "Current loss : 1.1101749555507006 Current Accuracy : 77.5 %\n",
      "Inputs processed : 950\n",
      "Current loss : 1.159516064686287 Current Accuracy : 76.5 %\n",
      "Inputs processed : 975\n",
      "Current loss : 1.0657679573286727 Current Accuracy : 78.4 %\n"
     ]
    }
   ],
   "source": [
    "loss_list = []\n",
    "accuracy_list = []\n",
    "for i in range(X.shape[1]):\n",
    "    grad_hidden_layer, grad_output_layer = model.backward(X[:,i],y[:,i])\n",
    "    model.internal_weights -= model.learning_rate * grad_hidden_layer\n",
    "    model.output_weights -= model.learning_rate * grad_output_layer\n",
    "    # prediction = model.predict(X)\n",
    "    # loss_list.append(compute_loss(prediction,y))\n",
    "    if i%25 == 0:\n",
    "        print(f\"Inputs processed : {i}\")\n",
    "        prediction = model.predict(X)\n",
    "        hits = np.sum([np.array_equal(prediction[:,i],y[:,i]) for i in range(1000)])\n",
    "        loss_list.append(compute_loss(prediction,y))\n",
    "        accuracy_list.append(hits*100/1000)\n",
    "        print(f\"Current loss : {compute_loss(prediction,y)} \\nCurrent Accuracy : {hits*100/1000} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3oAAAFACAYAAADqLQ6aAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAABXrUlEQVR4nO3dd1QUZxsF8Luw9CqCGjtgRBOxgFgxFlRQ1BCxIIoaMZagYhfs2LFGiRg1GnvEHow9aowVsSu2iNgrKGVBpM33h2YTPkXXgd2B5f7O2cMyMzvzzPVFeHZmZ2SCIAggIiIiIiIiraEjdQFERERERERUsNjoERERERERaRk2ekRERERERFqGjR4REREREZGWYaNHRERERESkZdjoERERERERaRm51AWI9fx5itQlvKNECWO8fJkmdRlFDnMTh7mJw9zEY3biMDdxmJs4zE0c5iYOcxOnIHOzsTHLcx6P6BUguVxX6hKKJOYmDnMTh7mJx+zEYW7iMDdxmJs4zE0c5iaOpnJjo0dERERERKRl2OgRERERERFpGTZ6REREREREWoaNHhERERERkZZho0dERERERKRl2OgRERERERFpGTZ6REREREREWoaNHhERERERkZZho0dERERERKRl5FIXoE0iIyOxadNWlChhhZIlrVGyZElYW1vD2trm7ffWMDY2lrpMIiIiIiLScmz0CtCsWbNw8uTJDy5jbGyMkiWtYW1trWz+/nnY2NigZMmSyu+trW1gYmKioeqJiIiIiEhbsNErQJs2bcLmzTuQkBCP+Pj4t1+fIyEhAQkJb75PS0tDWto93L9/76Pr09XVRUjIdPTr970GqiciIiIiIm3BRq8AlS9fHj169MpzviAIUChSlE1gQkLC20bw/Y3hw4cPEBIyAU2btoCDQzUN7gkRERERERVlGm30EhIS0LFjR6xcuRL29vbK6YcOHcLixYshl8vh7e2NLl26aLIsjZHJZDAzM4eZmTlsbe0+uvzIkUOxZs1KjBwZiN9+2wMdHV47h4iIiIiIPk5jnUNmZiYmTpwIQ0PDd6bPnDkTK1euxNq1axEREYH4+HhNlVWojR8/CTY2pRAVdRIbNqyVuhwiIiIiIioiNNbohYaGwsfHB6VKlco1PTY2FhUrVoSFhQX09fXh7OyM6OhoTZVVqFlalsC0abMAACEhE/Ds2TOJKyIiIiIioqJAI43etm3bYGVlhSZNmrwzT6FQwMzMTPm9iYkJFAqFJsoqEry8vNG8uRuSkhIxadJYqcshIiIiIqIiQCYIgqDujXTv3h0ymQwymQzXrl1D5cqVsWTJEtjY2OD69euYN28eli9fDgCYMWMGnJyc4OHh8cF1ZmVlQy7XVXfphcLt27fx5ZdfIj09Hfv370erVq2kLomIiIiIiAoxjTR6/+Xn54fJkycrL8aSmZkJT09PbNq0CcbGxvDx8cGSJUtQunTpD67n+fMUTZT7SWxszNRW16JF8zFt2mRUrmyLI0dOwcjISC3bkYI6c9NmzE0c5iYesxOHuYnD3MRhbuIwN3GYmzgFmZuNjVme8yS7jOPOnTsREREBPT09BAUFwd/fHz4+PvD29v5ok1ccDRw4GNWrf4E7d+Lwww9zpC6HiIiIiIgKMY0f0SsohfHdA3W/qxEdHQVPz1aQy+U4dOg4qlWrrrZtaRLfDRKHuYnD3MRjduIwN3GYmzjMTRzmJg5zE0frj+jRp3NxqY9evfyRlZWFkSMDkZOTI3VJRERERERUCLHRK2L+ubfe6dOnsH79GqnLISIiIiKiQoiNXhFjYWGJ6dNDAQBTpkzkvfWIiIiIiOgdbPSKoK+/7ogWLVoiKSkREycGS10OEREREREVMmz0iiCZTIbQ0PkwMjLCtm2bcfjwQalLIiIiIiKiQoSNXhFVqVJljBgRBAAYPXoYXr16pbFtP336BNOnh+Dq1RiNbZOIiIiIiFTHRq8IGzhwEKpX/wJ3797B/PmzNbLNc+fOoFWrpli4cB78/LpCoVBoZLtERERERKQ6NnpFmJ6eHubOXQiZTIbFixfi2rWrat3exo3r8fXXbfDkyWPI5XLcv38Ps2fPUOs2iYiIiIjo07HRK+Le3Fuvj1rvrZeVlYXx48dgyJCBeP36NXr39sfOnfugo6ODZcvCceHCuQLfJhERERERicdGTwuMGzcJpUqVRnR0FNatW12g637xIgFdu36DZcuWQE9PD/PmLcLs2Qvg7OyC/v0DkJOTg+HDhyArK6tAt0tEREREROKx0dMC/7233tSpk/D06dMCWW9MzBW0bt0cR48egY1NKWzbtgt+fr2V80ePHosKFSriypVLWLo0vEC2SURERERE+cdGT0t06PAN3NxaISkpEZMm5f/eejt37oCnZ0vcu3cHtWvXwYEDR1C/foNcy5iYmGD27PkAgNmzp+Pu3Tv53i4REREREeUfGz0tIZPJMGvWvLf31tuCQ4f+ELWenJwczJw5Bf7+PZGWlobOnX3w2297UbZsufcu7+bWGh07dsKrV68watRQCIKQn90gIiIiIqICwEZPi1SqVBkjR745mjd69HCkpaV90uuTk5PQs6cPFiyYCx0dHUyZMgM//rgURkZGH3zd1KmhsLS0xJ9/HsLWrZtE109ERERERAWDjZ6WGTAgANWrf4l79z7t3nqxsX+jTRs37N+/F5aWlti4cRsGDBgEmUz20dfa2NggJOTNbRYmTAjCixcJousnIiIiIqL8Y6OnZd7cW+8HyGQyhIcvwtWrMR99zcGD++Hu3gJ//30T1apVx759f6JZsxaftF0fn+5o3LgJEhISMHnyeLHlExERERFRAWCjp4VUvbeeIAhYtGgBfH07Izk5CW3btsfu3X/A1tbuk7cpk8kwb95CGBgYYOPG9fjrrz/zuRdERERERCQWGz0tNX78ZJQqVRpnzpzG2rWr3pmflpaGAQP6YNq0SRAEAWPGjMPKlWthamomept2dlUwYsQYAMDIkYF49eqV6HUREREREZF4bPS0lLm5BWbMePMZvf+/t979+/fQrl1rbN++FSYmpli9+leMGDEGOjr5Hw7ffz8E1at/gTt34j7pM4JERERERFRw2OhpsfbtvdCyZWskJydh4sQgAMCJE8fQunVTXLlyCba2dtiz5yDatPEssG3q6+tj7tyFkMlkWLx4IWJirhTYuomIiIiISDVs9LTYf++tt337VowYEYhOnTogISEBzZu7Yd++w6hWrXqBb9fFpT6+/bYvsrKyMGLEYGRnZxf4NoiIiIiIKG9s9LRcxYqVMGrUWADA2rW/ICsrCwEBgdiwYQssLUuobbvjxk3CZ5+VxblzZ/HLL8vVth0iIiIiInoXG71ioH//71G3bj0YGRlhyZKfMWnSVOjq6qp1m2Zm5pg1ax4AYPr0KXj48IFat0dERERERP9io1cM6OnpYceO3bh69Ta8vbtobLtt2njC07MDUlMVCAoaAUEQNLZtIiIiIqLijI1eMaGvrw8TExONb3fGjNkwMzPHvn178Pvvv2l8+0RERERExREbPVKrzz4ri/HjJwMAgoNHISkpUdJ6iIiIiIiKAzZ6pHa9evWBi0t9PHv2FFOnTpa6HCIiIiIircdGj9ROR0cH8+Ytgp6eHtasWYlTp05KXRIRERERkVZjo0caUa1adQwePAwAMGLEYLx+/VriioiIiIiItBcbPdKYoUNHwt6+Cv7++yYWLZovdTlERERERFqLjR5pjKGhIebNWwQAWLhwHm7evCFxRURERERE2omNHmlUo0au6NGjFzIyMjByZCBycnIKdP0vX77AiRPHoFAoCnS9RERERERFCRs90riJE6fAxqYUTp06gXXrVotejyAIuHXrb/z66zoMHz4Yrq4ucHCoDC+vtujTpwdv0E5ERERExZZcUxvKzs7G+PHjERcXB5lMhpCQEFStWlU5f9WqVdi8eTOsrKwAACEhIbCzs9NUeaRBlpYlMH16KPr1+xZTpkyEr29nyOWmH33dq1evcPHieZw+HYXo6FOIjo7Cixcvci1jYGAAHR0d/PnnIWzevBFdunRT124QERERERVaGmv0Dh8+DADYuHEjoqKisGDBAixZskQ5/8qVKwgNDUWNGjU0VRJJ6OuvO2LTpl/xxx/7ERgYiMWLV7yzzNOnT942dW8au0uXLiIzMzPXMjY2pVCvXgPUq9cALi71ULNmbWzbthlDhgzExInBcHNrjZIlS2pqt4iIiIiICgWNNXotW7ZEs2bNAACPHj2Cubl5rvkxMTFYtmwZnj9/jmbNmqF///6aKo0kIJPJEBo6H02a1MfmzZvRrl1HVKhQEdHRUTh9+hSio0/j3r0777zmiy9qoF69+nBxefOoVKkyZDJZruW6dvXF5s0ROHr0T0ycGIzFi5dpbL+IiIiIiAoDjTV6ACCXyzFmzBgcOHAAixYtyjXP09MTvr6+MDU1xaBBg3D48GE0b95ck+WRhlWoUBHBweMxYUIwevV69xRLExNTODu7KBu7unVdYGZm/p415SaTyTBnzgI0a9YQmzdvROfOPmjWrIU6doGIiIiIqFCSCRJcseL58+fo0qULdu3aBWNjYwiCAIVCATMzMwDA+vXrkZiYiICAgDzXkZWVDblcV1Mlk5pkZ2ejadOmOH78OCpVqoTGjRujUaNGaNy4MRwdHaGrK/7feNasWQgODoadnR0uX74MY2PjAqyciIiIiKjw0tgRvR07duDp06fo378/jIyMIJPJoKPz5qKfCoUC7dq1w+7du2FsbIyoqCh4e3t/cH0vX6ZpouxPYmNjhufPU6Quo8g5dOgQYmMfwtraOtf0Fy/y92/cs2c/rF27HlevXkFQ0HhMmBCSr/UVNhxv4jA38ZidOMxNHOYmDnMTh7mJw9zEKcjcbGzM8pynsdsrtG7dGlevXkX37t3h7++PsWPH4sCBA4iIiICZmRmGDRuGnj17wtfXF1WqVEHTpk01VRpJTF9f/50mryDo6elh/vxFkMlkCA9fhMuXLxX4NoiIiIiICiONHdEzNjbGwoUL85zv5eUFLy8vTZVDxYSTU1307dsfy5f/hBEjBmPPnkP5Oh2UiIiIiKgo4A3TSesFB09A2bLlcOHCeaxYsVTqcoiIiIiI1I6NHmk9U1MzhIbOBwDMmDEVDx7cl7giIiIiIiL1YqNHxYK7ext06PAN0tJSMWbMcEhwsVkiIiIiIo1ho0fFxvTpoTA3t8CBA/sQGbld6nKIiIiIiNSGjR4VG6VLl8HEiVMAAGPHjkZi4kuJKyIiIiIiUg82elSs9OjRCw0aNMLz588wZcpEqcshIiIiIlILNnpUrOjo6GDu3IXQ19fHunWrceLEMalLIiIiIiIqcGz0qNipWtUBgYEjAAAjRwYiPT1d4oqIiIiIiAoWGz0qloYMGY7PP6+KW7f+xsKF86Quh4iIiIioQLHRo2LJwMAA8+aFAQAWLZqP69evSVwREREREVHBYaNHxVaDBg3Rs2cfZGZmYsSIIcjJyZG6JCIiIiKiAsFGj4q1CRMmo1Sp0oiOjsKaNb9IXQ4RERERUYFgo0fFmoWFJWbOnAMAmDp1Ep48eSxxRURERERE+cdGj4q9du2+hrt7G6SkJCM4eJTU5RARERER5RsbPSr2ZDIZZs2aBxMTU+zaFYndu3+XuiQiIiIionxho0cEoFy58hg3biIAIDh4JFJSkiWuiIiIiIhIPDZ6RG99++13cHJyxuPHjzB9eojU5RARERERicZGj+gtXV1dzJsXBrlcjl9++RnR0VFSl0REREREJAobPaL/+PLLGggICIQgCBg5MhAZGRlSl0RERERE9MnY6BH9n+HDR6NyZVtcu3YV4eGLpC6HiIiIiOiTyaUugKiwMTIywty5C9GpUwfMmxeK9u2/hr395xqvIyMjAy9eJCA+Ph4JCfGIj3+OhIR/nicgJeUlTEzMMWPGHBgZGWm8PiIiIiIqvNjoEb3HV181Q9euvoiI2IABA/qidWsP6OvrQy7Xg56eHHp6+tDT04NcLoe+/j/P9aCvr/d2mX8fb6brQy6XQy6XIykpSdmwvWna/v36bzOXgKSkRJVqtbW1x5Ahw9QbCBEREREVKWz0iPIQEjIdf/yxDxcvnsfFi+c1vn0dHR1YWZWEjY0NSpa0fvsoCWvrN9/LZFkYPXo0Fi2ajx49esLKqqTGayQiIiKiwomNHlEerKxKYtOm33DgwF5kZGQgKyvr7ddMZGZmITMzA5mZmcjKykRGRubb6e9/vFkmA9nZ2TAzM3vbrJVEyZLWysbtzXNr5TxLyxLQ0cn7Y7Q2NmbYtWsPjhw5jIUL5yMkZLoG0yEiIiKiwoyNHtEHODrWhKNjTanLyNPEiVPg5tYEK1YsRd++/VGhQkWpSyIiIiKiQoBX3SQqwhwda6Fjx87IyMjArFnTpC6HiIiIiAoJlY/o3b17FzExMe+9r5iXl1dB1kREnyA4eAJ27tyBLVsiMHDgYNSo4Sh1SUREREQkMZUavRUrVmDevHkQBCHXdEEQIJPJ2OgRSahSpcr49tu+WLZsCaZNm4SNG7dJXRIRERERSUzlRi8nJwdOTk6ws7ODrq6uuusiok8wbNhobNiwDocO/YGjR4+gSZOmUpdERERERBJSqdHLzs5GzZo1sWHDBnXXQ0QilCxZEoMHD8XMmVMxdepE7N17+INX7CQiIiIi7abSX4I+Pj549uwZEhIS1F0PEYnUr9/3KF26DC5cOI+dO3dIXQ4RERERSUilI3opKSlQKBRwd3dHtWrVYGhoCJlMBgCQyWRYtmyZWoskoo8zMTHB6NFjMWLEEEyfHoI2bdpBX19f6rKIiIiISAIqNXr/PWXzzJkzueb90/ARkfS6deuBn376EX//fRNr166Cv38/qUsiIiIiIgmo1OitWbMm3xvKzs7G+PHjERcXB5lMhpCQEFStWlU5/9ChQ1i8eDHkcjm8vb3RpUuXfG+TqLiRy+UYN24yevf2xbx5s9C1azeYmppJXRYRERERaZhKjV69evUAABkZGbh58yZkMhk+//zzTzot7PDhwwCAjRs3IioqCgsWLMCSJUsAAJmZmZg5cya2bNkCIyMjdOvWDS1atIC1tfWn7g9RsdemjSdcXOojOjoKixcvwpgx46QuiYiIiIg0TOXL8q1YsQINGjRA586d0alTJzRq1AirVq1SeUMtW7bE1KlTAQCPHj2Cubm5cl5sbCwqVqwICwsL6Ovrw9nZGdHR0arvBREpyWQyTJgwBQCwZMmPePr0qcQVEREREZGmqdTo/frrr5gzZw4yMjJQs2ZN1KxZE+np6QgNDcXmzZtV3phcLseYMWMwdepUtG/fXjldoVDAzOzf08tMTEygUCg+YTeI6L8aNGgID4+2SEtLxbx5s6Quh4iIiIg0TCYIgvCxhdzd3ZGQkICNGzeiSpUqAIC///4bPj4+sLGxwd69ez9po8+fP0eXLl2wa9cuGBsb4/r165g3bx6WL18OAJgxYwacnJzg4eGR5zqysrIhl/PG7UR5uXr1KhwdHSGTyXD16tVcn4klIiIiIu2m0mf0Hj16BGdnZ2WTBwCff/45HB0dce7cOZU2tGPHDjx9+hT9+/eHkZERZDKZ8obO9vb2uHv3LhITE2FsbIwzZ87A39//g+t7+TJNpe1qko2NGZ4/T5G6jCKHuYnzsdxsbCqgW7ceWL9+DUaOHIMVK/J/USVtwPEmHrMTh7mJw9zEYW7iMDdxmJs4BZmbjU3eF91T6dTNcuXK4cqVK4iNjVVOi42NxeXLl1GhQgWVimjdujWuXr2K7t27w9/fH2PHjsWBAwcQEREBPT09BAUFwd/fHz4+PvD29kbp0qVVWi8R5W306LEwNDTEzp07cPYsP/dKREREVFyodESvZ8+emDJlCry8vPDll18CAGJiYpCVlQVfX1+VNmRsbIyFCxfmOb9FixZo0aKFSusiItV89llZ9Ov3PRYtmo+pUydh+/ZdvPclERERUTGg0hE9X19fDB8+HHp6erhw4QIuXLgAuVyOAQMGoHv37uqukYjyYfDgoShRogROnDiGP/7YJ3U5RERERKQBKh3RA4B+/fqhV69euHXrFnR0dGBrawtDQ0N11kZEBcDCwhLDho3CxIljMW3aZLRo0Qq6uryQEREREZE2y7PR2717N8qUKQMnJyfs3r37nflxcXHK523btlVPdURUIL799jssX/4Trl27is2bN8LHh0fiiYiIiLRZno3e8OHD0apVKzg5OWH48OEf/FwPGz2iws3AwABBQeMRENAPs2ZNw9dfd4SRkZHUZRERERGRmuTZ6Hl5eaFGjRrK57yAA1HR5u3dBeHhYYiJuYwVK5Zh0KBAqUsiIiIiIjXJs9GbNWvWe5//V0ZGBhQKRcFXRUQFTkdHBxMmhMDHpyMWLpyHHj16wtKyhNRlEREREZEaqHTVzerVq2Pw4MHvTO/Zsyc6depU4EURkXo0b+6GJk2aIikpEQsXzpe6HCIiIiJSkzyP6G3ZsgX79r25FLsgCDh//jy+++475fycnBxcu3aNV+8jKkJkMhkmTAhB69bN8PPPP8Hfvx/Kl68gdVlEREREVMDyPKLXokULXLp0CUePHoVMJkN8fDyOHj2qfBw/fhyvX7+Gu7u7JuslonyqXdsJXl4d8fr1a8yePUPqcoiIiIhIDfI8omdlZYWNGzfi2bNn6NWrF5ycnDB06FDlfB0dHZQoUQL29vaaqJOIClBw8ET8/nskIiI2YMCAQfjiiy+lLomIiIiICtAHb5hua2sLW1tbHDx4EEZGRrCwsFCeqpmcnAxzc3ONFElEBcvW1g69evXBihXLMH36ZKxfv1nqkoiIiIioAKl0MRYLCwtMnjwZU6ZMUU5r164dAgICkJKSorbiiEh9hg8fAxMTUxw4sA8nThyTuhwiIiIiKkAqNXrTpk3D/v378ejRIwDA69evoaenh0OHDmHmzJlqLZCI1MPGxgYBAUMAAFOmTIAgCBJXREREREQFRaVG78iRI7C3t0d4eDgAwMDAAHv37oWdnR2OHDmi1gKJSH0GDBgEG5tSOHfuLH7//TepyyEiIiKiAqJSo/fq1SsYGxtDT09POU1PTw+GhoZITU1VW3FEpF6mpqYYNSoYADB9eggyMzMlroiIiIiICoJKjZ6zszOuXLmC0aNHY9u2bdi8eTMCAwNx9epV1KlTR901EpEade/eE/b2VXD7dizmzJmJjIwMqUsiIiIionxSqdEbN24cypQpg8jISIwbNw4TJ07Evn37YGNjg3Hjxqm7RiJSIz09PYwfHwIA+OGHuWjUqC4iIjYgOztb4sqIiIiISKwP3l7hH3Z2dti7dy8iIyMRGxsLALC3t0e7du1gZGSk1gKJSP08PdtjxYq1mDlzCm7d+huDBw9AWNgCjBkzDp6eHaCjo9J7QkRERERUSKjU6AFvLsDSuXNnddZCRBJq3/5rtGnjiS1bIjB37izcvHkD/v494ehYC0FB49CypTtkMpnUZRIRERGRClRq9JKTk7Fy5UrExMQgIyMj12XYZTIZVq9erbYCiUhz5HI5fHy6o2PHzli/fg3mz5+Ny5cvonv3LnBxqY/g4Alwdf1K6jKJiIiI6CNUavRGjx6NI0eOvPc+W3yHn0j76Ovr49tv+8LHpzt++eVnhIXNR3R0FDp2bIcmTZph7NgJcHZ2kbpMIiIiIsqDSo3eyZMnYWhoiLFjx8LOzg5yucpnfBJREWZkZITvvx+Mnj17Y+nScISHh+Ho0T/Rps2fcHdvgzFjxqNGDUepyyQiIiKi/6NSx1auXDnY2NjwM3pExZSpqRlGjBiDPn2+Q3h4GJYvX4J9+/Zg37498PLqiNGjx6FKlc+lLpOIiIiI3lLpUnoTJ07E9evX8euvv+L+/fuIj49HQkKC8kFExUOJElYYN24STp++hH79BkJfXx87dmyDq6sLAgO/x717d6UukYiIiIgAyIT3ffDu/zg5OSEjI+O999WSyWS4evWqWor7kOfPUzS+zY+xsTErlHUVdsxNnMKQ28OHDzB//mxs2LAW2dnZ0NPTg59fbwwbNgqlS5eRtLa8FIbciipmJw5zE4e5icPcxGFu4jA3cQoyNxsbszznqXREz9LSEqVKlcJnn332zqNMmcL5xxwRqV+5cuUxb94iHD9+Bt7eXZCVlYWVK5fDxaUmxo0bjT//PITU1FSpyyQiIiIqdlQ6olcYFcZ3D/iuhjjMTZzCmNv169cQGjodu3ZFKqfJ5XLUqeOMRo1c0aiRK1xc6sPU1FSyGgtjbkUFsxOHuYnD3MRhbuIwN3GYmziaOqKn0sVYHj169MH5ZcuW/bSKiEgrVatWHb/8sg6XLl3A9u1bceLEUVy8eAHR0VGIjo7CwoXzIJfLUbu2Exo3boKGDRujXr0GkjZ+RERERNpIpUavRYsWed4vT6rP6BFR4VWzZm3UrFkbAJCcnITTp0/h+PFjysbvzJnTOHPmtLLxq1WrDho3boJGjf5p/PJ+d4qIiIiIPk6lRq9y5crKRk8QBGRkZOD58+fQ09NDvXr11FogERVt5uYWaNnSHS1bugMAUlKS32n8zp6Nxtmz0Vi0aD50dXVRu3YdNGrUBI0bu7LxIyIiIhJBpUZv796970x78uQJevXqhQYNGhR4UUSkvczMzOHm1hpubq0BAApFSq7G78KF8zh79gzOnj2DsLAF0NXVRa1ateHv3x+dO/tIXD0RERFR0aBSo/c+ZcqUgaOjI1auXInevXsXYElEVJyYmpqhRYtWaNGiFYB/G78TJ47j+PGjuHDhHM6dO4tz5/rhyJHDCA2dDxMTE4mrJiIiIircVGr0li9fnut7QRDw7Nkz7N+/HwYGBmopjIiKp3cbPwW2bt2EiRODsWnTr7hw4RyWL1+N6tW/kLhSIiIiosJLpUZv3rx571yM5Z+7MvTs2fOjr8/MzMTYsWPx8OFDZGRkYODAgXBzc1POX7VqFTZv3gwrKysAQEhICOzs7FTeCSLSXqampujVqw/q12+Ivn174ubNG/DwaI4ZM+bA19cvzwtFERERERVnKjV6AQEB7/wxZWBggOrVq8PV1fWjr4+MjISlpSXmzJmDxMREeHl55Wr0rly5gtDQUNSoUeMTyyei4qJaterYt+9PBAePxMaN6zFs2CAcP34Us2cv4O0ZiIiIiP6PSo3e4MGD87URDw8PuLu/ueKeIAjQ1dXNNT8mJgbLli3D8+fP0axZM/Tv3z9f2yMi7WRiYoJFi5agUSNXBAWNwJYtEcpTOb/8km8UEREREf1DJ68Zbm5umDRpUq5p169fx8OHDz95IyYmJjA1NYVCocCQIUMwdOjQXPM9PT0xefJkrF69GmfPnsXhw4c/eRtEVHz4+HTHvn1/olq16rh162+0adMCa9euUp5STkRERFTcyYQ8/jKqVq0aWrZsiR9//DHXtFatWiEsLOyTN/T48WMEBATA19cXnTp1Uk4XBAEKhQJmZm/uk7V+/XokJiYiICDgg+vLysqGXK77wWWISLulpaVhyJAhWLFiBQCgW7duWLp0qfL/EyIiIqLi6pNvryDmHfP4+Hj06dMHEydORMOGDXPNUygUaNeuHXbv3g1jY2NERUXB29v7o+t8+TLtk+tQNxsbMzx/niJ1GUUOcxOHub0xc+YCODnVx6hRw/Drr78iKuo0fv55DWrUcHzv8sxNPGYnDnMTh7mJw9zEYW7iMDdxCjI3G5u839zO89TNgvTTTz8hOTkZ4eHh8PPzg5+fHyIjIxEREQEzMzMMGzYMPXv2hK+vL6pUqYKmTZtqoiwi0hKdO/vgwIEjqF79S9y+HYs2bVpg1aoVPJWTiIiIii3RN0z/FOPHj8f48ePznO/l5QUvLy9NlEJEWurzz6ti795DGD8+CGvX/oLRo4fhxImjmDdvEczMzKUuj4iIiEijPtjoHT58GLVq1VJ+L5PJ3jvtwoULaiuQiEhVRkZGmDdvIRo3dsWIEYHYsWMbLl68gJ9/Xg1Hx1ofXwERERGRlvjgqZvZ2dl4/fq18iEIwjvT0tPTNVUrEZFKOnbsjD/+OIIvv3REXNxttGnjhpUrl/NUTiIiIio28jyid/36dU3WQURUoOztP8eePQcxYUIwVq9egaCgETh+/CjWrl0FDX08mYiIiEgy/GuHiLSWoaEh5sxZgOXLV8HU1Aw7d+6Ak5MTLlw4J3VpRERERGrFRo+ItN7XX3fEH3/8BUfHWrh9+zbc3Zvj66/bYMWKpXj69InU5REREREVODZ6RFQs2NnZY9euAxgyZAj09PRw8uRxBAePQs2aDujQwQM///wTnjx5LHWZRERERAWCjR4RFRuGhoZYuHAhrl6NxeLFy+Dh0Rb6+vo4deoExo4djVq1qqF9e3csX74Ejx8/krpcIiIiItFUavR69uyJRYsWvTM9KCgI/v7+BV4UEZE6mZtboHNnH6xZsxFXr8YiPHw5PDw8oa+vj6iokxg3bgxq166Odu1aY9mycDZ9REREVOTkedXNI0eO4ObNmwCA06dP4+XLlzAyMlLOFwQBJ06cQFJSkvqrJCJSEzMzc3Tq1BWdOnVFSkoy9u/fi8jIHTh06ABOnz6F06dPYfz4ILi41EeHDl5o394LZcuWk7psIiIiog+SCXncWOrGjRvw9vZGdnb2e1/4z8ucnZ2xfv169VWYh+fPUzS+zY+xsTErlHUVdsxNHOYmjqq5KRQpuZq+/94ztLg2fRxz4jA3cZibOMxNHOYmDnMTpyBzs7Exy3Neno0eAOzatQu3b9/G4sWLYWtrC09Pz39fKJPBysoK7u7usLKyKpBCP0VhHFQc7OIwN3GYmzhiclMoUnDgwD5ERu7AwYP7czV9devWQ69efdCpU1fo6uoWdLmFCsecOMxNHOYmDnMTh7mJw9zE0VSjl+epmwCUjV358uXx2WefoUGDBgVSEBFRUWJqaoZvvumEb77pBIVCgT/++LfpO3PmNM6cOY3w8DBMmDAZbm6tIZPJpC6ZiIiIijmVLsbyzTffwNTUFE+fPgUArFu3DgMHDsQvv/yCDxwQJCLSOqampvDy8sbKlWsRExOLhQvDUb58BVy7FgNf38745htPnD0bLXWZREREVMyp1OhFRkaiS5cuOHbsGI4fP45p06bh8OHDmD17NsLDw9VdIxFRoWRqaopu3XrgxImzCAmZgRIlSuDEiWNo08YN/v49ERv7t9QlEhERUTGlUqO3dOlSGBgYoEKFCoiMjIShoSF++eUXlCxZEjt27FBziUREhZuhoSEGDhyE06cvIjBwBIyMjLBz5w64utbDqFHD8PTpE6lLJCIiomJGpUbvwYMHcHFxQb169XD8+HHUrl0bDRs2xBdffIFnz56pu0YioiLBwsIS48ZNwqlT59GjRy8IgoDVq1egfv3amDVrKlJSkqUuUXJpaWnYsiUC337bA6NHD8OpUyeRk5MjdVlERERaR6VGz9zcHI8ePcLRo0cRHx+PRo0aITExEdevX0fJkiXVXSMRUZHy2WdlMX9+GP76Kwpt2rRDWloa5s+fg3r1amH58iV4/fq11CVqlCAIOHXqJIYNG4QaNT7H999/h127IrFq1Qp06OAOF5eamDZtMq5ejZG6VCIiIq2hUqPXtGlT3Lp1C/369YOenh7atm2LoKAgPHv2DO3bt1d3jURERVLVqg5YvXoDfv/9AOrVa4CEhASMGzcGjRu7YOvWTVp/JOv+/XuYNy8U9evXRocO7li/fg0UihQ4O9fFzJlzMGjQUJQtWw7379/DokXz0axZQzRt2hCLFs3H/fv3pC6fiIioSPvgffT+kZaWhrCwMDx48AA+Pj5o3LgxfvjhBygUCgQHB0ty76jCeM8O3ktEHOYmDnMTR6rcBEHAvn17MH36ZNy4cR0AUKNGTUyYEILmzd00Xo8YqmSXmpqK33//DRERG3Ds2F/K6WXKfIYuXbqha1dffP55VeX0nJwcREWdxJYtm7Bz53YkJiYq59Wv3xAdO3ZGhw7fFOmzR/izKg5zE4e5icPcxGFu4hSKG6a/z8uXLyGTyWBpaZnfuvKlMA4qDnZxmJs4zE0cqXPLysrCpk2/IjR0Oh4/fgQAaNKkGSZODEGtWnVEr1cQBKSmKpCUlISkpCQkJ795lChhBTu7KrCyssr3/f3yyi4nJwenTp3Axo3rsXPnb0hNVQB4c5Gatm3boWvX7vjqq2YffVMwIyMDhw8fxLZtm7B37268evUKACCXy9G8uRs6duwMDw9PmJiY5Gs/NE3qMVdUMTdxmJs4zE0c5iZOoWv09uzZgwULFuD+/fsAgEqVKmHEiBFo1apVgRT5qQrjoOJgF4e5icPcxCksub169Qo//7wUCxfOQ3JyEgDAy6sjBg0aCh0d3beNWjKSkhKRnJyUq4H779c3zxORnJz8wVNBLSwsYWdnB1tbe9jbV4Gdnb3yYWFhqVLN/5/dnTtx2LTpV2zatBH37t1RTndxqQ8fn+74+utvYG5uISofhSIFe/bswtatm3DkyGFkZ2cDAIyNjeHh4Qlv785o1swNenp6otavSYVlzBU1zE0c5iYOcxOHuYlTqBq9ffv2ITAwEABgbW0NAIiPj4eOjg4WLVqEli1bFkihn6IwDioOdnGYmzjMTZzCltvLly+wcOF8rFixNN8XaTE2NoGFhQUsLCxgbm4BU1NTxMfH4/btWCgUee9zyZIl39sA2traw9TUVLmcjY0Z4uIeYefON6dmnjhxTDmvbNly6Nq1G7p06QZ7+8/ztR//7/nz54iM3I6tWzfhzJnTyulWVlbo0OEbdOzYBfXq1YeOjkofO9e4wjbmigrmJg5zE4e5icPcxClUjV6HDh1w9+5dhIeHo3HjxgCAY8eO4fvvv4ednZ0k99IrjIOKg10c5iYOcxOnsOb24MF9zJkzE8eO/QVTU1OYm//bsP3bvFm+/Wr+f/MtYW5unufRLUEQ8OzZM8TFxeL27TeP2NhbuH07Fnfu3FaeIvk+pUuXUTZ+QDa2b9+OtLQ0AICRkRE8PTuga1dfuLp+pZHPa9+5E4ft27dg69ZNuHnzhnJ6pUqVMXPmHLRs6a72Gj5VYR1zhR1zE4e5icPcxGFu4hSqRs/R0RHOzs5YtWpVrum9evXC+fPncenSpXwX+akK46DiYBeHuYnD3MRhbrnl5OTgyZPHuZq/fxrCO3fikJGR8c5rGjRohK5dfdGhgxfMzMwlqPpN8xoTcwVbt27C9u1b8OjRQwCAn19vhIRMh6lp3r/4NI1jThzmJg5zE4e5icPcxNFUoydXZQWlSpXCzZs38eLFC1hZWQEAXrx4gZs3b6J06dIFUiQREWmejo4OypYth7Jly8HV9atc87Kzs/HgwX3lUUC5XECTJi3fHt2TlkwmQ40ajqhRwxHjx09GeHgYQkOnYe3aVThy5E/8+ONPaNCgkdRlEhERSUalRs/b2xuLFi1CmzZt4OrqCgA4fvw4kpKS4Ofnp9YCiYhIGrq6uqhUqTIqVaqM5s3dCu07t7q6uhg8eCjc3FohIKAfYmIu4+uv22DgwMEIChoPQ0NDqUskIiLSOJU+uT5gwAB07doVycnJ2LVrF3bt2oXExES0b98e/fv3V3eNREREH/XFF19i377DGDZsJGQyGcLDF6F166a4dOmC1KURERFpnEpH9HR0dBASEoKBAwfi0qVL0NHRQbVq1VC+fHl110dERKQyfX19BAdPRKtWHhg0qD+uX78GD48WGDFiDAIDR0AuV+nXHhERUZH30SN6sbGxyMrKAgCUKVMGrVu3RrVq1WBuLs0H8ImIiD6mbt16OHToOPr27Y+srCyEhk6Hp2dL/P33TalLIyIi0og8G72cnByMHTsW7dq1w4ULF3LNW7BgAb766qt3rsJJRERUWBgbG2PGjDnYsiUS5cqVx/nz5+Dm5oply8I/eHN5IiIibZBno7dixQps27YN+vr6ePbsWa556enpyMjIQGhoKHbu3Kn2IomIiMT66qtmOHLkJLp29UV6ejrGjw9Cp04dcP/+PalLIyIiUps8G72tW7fCwMAAERERaNu2ba55ixcvxs8//wxdXV2sXr1a7UUSERHlh7m5BcLCfsKqVRtgbW2NY8f+QtOmDfHrr+ugwu1kiYiIipw8G72HDx/CyckJ1apVe+/8Ro0aoU6dOrh9+7baiiMiIipIbdu2w5EjUWjbtj0UihQEBn6PXr26vXPmChERUVGXZ6NnYWGBhw8ffvDFz549g76+/kc3kpmZiVGjRsHX1xedOnXCwYMHc80/dOgQvL290bVrV2zatEnF0omIiD6djY0NfvllHcLCfoKZmTn27t2Npk3rY+fO36QujYiIqMDk2eg1btwY9+/fx7Rp05Cenp5rXnp6OqZNm4Z79+6hcePGH91IZGQkLC0tsWHDBvz888+YOnWqcl5mZiZmzpyJlStXYu3atYiIiEB8fHw+domIiOjDZDIZunb1xZEjJ9GkSTMkJCTA398P33//HZKSEqUuj4iIKN/yvKFQYGAg/vrrL6xfvx5bt26Fra0tTExMoFAoEBcXh9evX8Pc3BxDhw796EY8PDzg7u4OABAEAbq6usp5sbGxqFixIiwsLAAAzs7OiI6ORps2bfK5a0RERB9WvnwFbN68A7/8shxTpkzEli0ROH78KH74YTGaN3eTujwiIiLR8jyiV7ZsWURERKBBgwZ49eoVrl69iujoaFy7dg3p6elwcnLC+vXrUaFChY9uxMTEBKamplAoFBgyZEiu5lChUMDMzCzXsgqFIn97RUREpCIdHR34+/fHoUPH4OxcF48fP0LXrt9gzJjhSE1Nlbo8tUlOTsL582eRmZkpdSlERKQGMkGFy409ffoU165dQ0pKCoyNjeHg4IDy5ct/0oYeP36MgIAA5ef0/nH9+nXMmzcPy5cvBwDMmDEDTk5O8PDw+OD6srKyIZfrfnAZIiKiT/Hm5uqhCAkJQWZmJqpUqYLVq1ejUaNGUpdWYBQKBcLCwjB79mwkJiaidOnS6N27N/r27YsqVapIXR4RERUQlRq9/IqPj4efnx8mTpyIhg0b5pqXmZkJT09PbNq0CcbGxvDx8cGSJUtQunTpD67z+fMUdZYsio2NWaGsq7BjbuIwN3GYm3jFKbvLly9h0KB+uHbtKnR0dDBo0FCMGhUMAwODT15XYcktPT0dq1evwMKF8xEf/xwAYG1to3wOAK6uX6FHj15o27Y9DA0NpSoVQOHJrahhbuIwN3GYmzgFmZuNjVme8/I8dbMg/fTTT0hOTkZ4eDj8/Pzg5+eHyMhIREREQE9PD0FBQfD394ePjw+8vb0/2uQRERGpk6NjTezffwSDBw+DIAhYtGg+3N2bIybmitSlfbLMzEysWfMLGjSogwkTghEf/xxOTs7YvPk3xMTcws6d++Hj0x1GRkY4duwvDBjgj1q1HDBhQhBu3LgudflERCSSRo7oqUNhfPeA72qIw9zEYW7iMDfximt2UVGnMHhwf9y5Ewc9PT2MHj0WAQGBkMvzvJ5ZLlLllp2djW3bNmPOnJm4cycOAPDFFzUQFDQe7u5tIJPJci2fnJyErVs3Y9261bh8+aJyuotLffj59UaHDt/A2NhYY/UX1/GWX8xNHOYmDnMTR6uO6BERERVV9es3wKFDx9G7tz8yMzMxfXoIOnTwwO3bt6Qu7b0EQcDvv0eiWbOGCAjohzt34mBvXwXLlv2CQ4eOwcOj7TtNHgCYm1vg22/74uDBozhw4Ah69uwDU1MzREdHYciQgXB0rIrRo4flagKJiKjwYqNHRET0Eaamppg9ewE2btyGMmU+w5kzp9GihStWrlyOwnJijCAIOHhwP1q1aoo+fXrgxo3rqFChIhYuDMfRo6fh5eUNHR3Vfu3XqlUHc+f+gEuXbuCHHxbD2dkFKSnJWLVqBdzcmqBly6+watUKpKQkq3mviIhILDZ6REREKmrRoiX++usUOnbsjLS0NAQFjUCXLl549OihpHWdOHEM7du7o1u3Trh06QJKlSqNmTPn4sSJs+jWrYfKp5n+P1NTU/j6+mHPnoP488+T+O67AbC0tMSlSxcwevQwODpWRWDg94iOjio0DS8REb3Bz+gVIJ6nLA5zE4e5icPcxGN2ue3cuQOjRg3FixcvYG5ugRkzZqNzZ593TotUZ27nzp3BzJlTceTIYQCAlZUVBg8ejm+/7au2z9Olp6fj999/w7p1q3HixDHl9OrVv0CrVh6ws7OHnV0V2NnZw8bG5r2niaqC400c5iYOcxOHuYmjqc/osdErQBzs4jA3cZibOMxNPGb3rqdPn2LEiMHYv38vAMDTswPmzPkB1tbWymXUkVtMzBWEhk7H3r27AABmZuYYOHAQ+vf/HmZm5gW6rQ+Jjf0b69atQUTEesTHx78z39TU7G3jZ5erAbSzs4eVVckPrpvjTRzmJg5zE4e5icNG7yMK46DiYBeHuYnD3MRhbuIxu/cTBAG//roO48aNQWqqAtbWNpg/PwweHm0BFFxuCoUCcXGx+PHHH7BjxzYIggAjIyP07TsAAQFDPto4qVNGRgYOHfoDly9fxO3bsYiLi0VsbCySkhLzfI2lpeU7zd8/D3NzC443kZibOMxNHOYmDhu9jyiMg4qDXRzmJg5zE4e5icfsPuzevbsYMmSg8nRGH5/umDZtFuzty38wt8zMTDx79hRPnjzG48eP8fTpm6+PHz/CkydP8OTJm6//vfCJvr4+evXqgyFDRhTae88KgoAXL17g9u1buH079u3X22+fxyI1VZHna62trVGjRg107uwLLy9v6OnpabDyoo0/p+IwN3GYmzhs9D6iMA4qDnZxmJs4zE0c5iYes/u4nJwcLF++BNOmTcbr169RvnwFLFq0EBkZAp48eYLHjx/lauaePHmM58+fqXQhEwMDA5Qu/RmaNm2OYcNGonz5ChrYI/UQBAHPnj1VNn3/Pm4hLu420tPTlcuWK1ce/fp9Dz+/XjA1zfsPGnqDP6fiMDdxmJs4bPQ+ojAOKg52cZibOMxNHOYmHrNT3c2bNzBoUD9cuHD+o8vKZDLY2JTCZ5+VRZkyZVCmzJuv//99iRJWoi9sUpTk5OTg8eNHOHPmOEJDZ+PWrb8BvLnPX69effDddwNQpsxnEldZePHnVBzmJg5zE4eN3kcUxkHFwS4OcxOHuYnD3MRjdp8mMzMTYWELsG/fLpiZWb63efvss7KwsSnFUxPfw8bGDE+fJuHAgX1YvHghTp06AQDQ09NDp05d8f33Q+DgUE3iKgsf/pyKw9zEYW7isNH7iMI4qDjYxWFu4jA3cZibeMxOHOYmzv/ndvZsNBYvXoRduyKVp7q2auWOgIBANGzYuFgc7VQFx5s4zE0c5iaOpho93jCdiIiICj1nZxesXLkWJ0+eQ+/e/jA0NMSBA/vg5dUWHh7NERm5HVlZWVKXSURUaLDRIyIioiLDzs4es2cvwLlzVzFyZBCsrKxw/vw59O3bCw0aOGHFimVIS0uTukwiIsmx0SMiIqIix9raGqNHj8W5c1cxa9Y8VK5si3v37iA4eCScnL7ArFnT8Pz5c6nLJCKSDBs9IiIiKrKMjY3Rp893OHnyHFasWAtn57p48eIF5s+fDWfnLzFy5FDcvn1L6jKJiDSOjR4REREVebq6umjf/mvs3n0QkZF74e7eBunp6VizZiUaNnRGnz5+uHjx47e7ICLSFmz0iIiISGvIZDI0aNAIa9dG4NixaHTv3hN6enr4/fff0KpVU3Tt+g1OnjwudZlERGrHRo+IiIi0UtWqDliw4EecPXsF338/BMbGJjh8+CC+/roN2rd3x8GD+1FE7zJFRPRRbPSIiIhIq5UuXQaTJ0/DuXNXMHJkECwtLREVdRLdunWCm1sTREZuR3Z2ttRlEhEVKDZ6REREVCxYWZV8e6XOGEyaNA2lSpXGlSuX0LdvL7i6uuDXX9chMzNT6jKJiAoEGz0iIiIqVkxNzRAQMARnzlzG7NkLULFiJcTG3kJg4PeoX782VqxYilevXkldJhFRvrDRIyIiomLJ0NAQvXv74+TJc/jxx6WoWtUBDx7cR3DwKDg718CiRfORkpIsdZlERKKw0SMiIqJiTU9PD126dMNff0Xhl1/Wo1atOoiPf45p0yajTp0vMWvWVCQkJEhdJhHRJ2GjR0RERARAR0cHnp7tsX//n4iI2I5GjVyRnJyE+fPnwNn5S0yYEIRHjx5KXSYRkUrY6BERERH9h0wmQ/PmbtixYzd27tyPli1bIy0tDUuXhsPFpSaGDBmINWt+wfHjR/H48SPeooGICiW51AUQERERFVb16zfAhg1bcPnyJSxaNB+RkduxceN6bNy4XrmMsbExKle2g719FdjZ2cPOzh62tvawt68Ca2tryGQyCfeAiIorNnpEREREH+HoWBPLl6/CmDHjsHPnDsTG3kJs7C3ExcXixYsXuHr1Cq5evfLO68zMzN82f3aws/u3EbSzs0eJElYS7AkRFRds9IiIiIhUVKXK5xg2bFSuaYmJL3H7dux/Hrfefr2N5OQkXLx4Hhcvnn9nXVZWVrC1tYeLS3107eqLL7+soandIKJigI0eERERUT5YWpaAk1NdODnVzTVdEAQkJCT8X/P37+PFixd48eIFzp6Nxk8//YgaNWrCx8cXHTt2gbW1tUR7Q0Tago0eERERkRrIZDJYW1vD2toa9erVzzVPEAQ8e/YUN2/ewO+//4bt27fgypVLGD/+EiZPHo+WLd3h49MdLVu2hr6+vkR7QERFGa+6SURERKRhMpkMpUuXQZMmTREaOh+XL/+NFSvWoFUrdwiCgL17d6F3b1/UquWAceNG4/Lli7y6JxF9EjZ6RERERBIzMDBA+/ZeWL9+My5cuIZJk6ahWrXqSEhIwPLlP8HNrQmaN2+MJUt+xLNnz6Qul4iKADZ6RERERIVI6dJlEBAwBEeOnMKBA0fg798PJUqUwNWrVzBp0ljUquUAP7+u+P33SGRkZEhdLhEVUhpt9C5evAg/P793pq9atQqenp7w8/ODn58fbt++rcmyiIiIiAodmUyGWrXqYObMubh06SZWrlwHd/c2AIB9+/agT58eqFmzKoKDR+LixfM8tZOIctHYxViWL1+OyMhIGBkZvTPvypUrCA0NRY0avKwwERER0f8zMDBAu3Yd0K5dBzx79gzbtm3Cxo0bcPXqFaxYsQwrVixDtWrV4e/fB15eXWFhYSl1yUQkMY0d0atYsSLCwsLeOy8mJgbLli1Dt27dsHTpUk2VRERERFTklCpVCgMGDMKff57AwYPH0K/fQJQsWRLXr1/DqFGj4OzsiDlzZiIpKVHqUolIQhpr9Nzd3SGXv/8AoqenJyZPnozVq1fj7NmzOHz4sKbKIiIiIiqyHB1rYtq0UFy8eAOrVm1As2bNkJychDlzZsLZ2RGzZ89gw0dUTMkEDZ7Q/eDBAwwfPhybNm1SThMEAQqFAmZmZgCA9evXIzExEQEBAR9cV1ZWNuRyXbXWS0RERFTUHDlyBCEhIco3zi0sLBAYGIihQ4eiRIkSEldHRJoi+Q3TFQoF2rVrh927d8PY2BhRUVHw9vb+6OtevkzTQHWfxsbGDM+fp0hdRpHD3MRhbuIwN/GYnTjMTRzmJo6NjRm++MIJERG/4eTJ45g7dxaOHj2CKVOmYMGCH/DddwMwYEAALC3Z8P0Xx5s4zE2cgszNxsYsz3mS3V5h586diIiIgJmZGYYNG4aePXvC19cXVapUQdOmTaUqi4iIiEgrNGzYGFu37kRk5F40adIMKSnJmD9/NpydHTFr1jS8fPlC6hKJSI00eupmQSqM7x7wXQ1xmJs4zE0c5iYesxOHuYnD3MT5UG6nTp3E3Lmz8Ndfb07pNDU1Q79+A9C/fwBKlLDSZJmFDsebOMxNHK0/okdEREREmtOgQUNs2fIbdu7cj6ZNm0OhSMH8+XPg7OyImTOn4MWLBKlLJKICxEaPiIiIqBipX78BNm/+Db//fgDNmrWAQpGCBQvmwtnZETNmsOEj0hZs9IiIiIiKoXr16mPTph3YtesAmjd3Q2qqAj/88Kbhmz49BAkJbPiIijI2ekRERETFmItLfUREbMfu3X+gRYuWSE1VYOHCeahb1xEBAf2wYMEcREZux+XLl6BQKKQul4hUJPntFYiIiIhIenXr1sPGjdtw9mw05s6dhYMHD2Dz5o3vLFe6dBnY2dnD3r4KbG3tYWf35lG5si2MjIwkqJyI3oeNHhEREREpOTu74Ndft+LKlcs4ezYat2/HIi4uFrdvx+LOnTg8ffoET58+wcmTx3O9TiaToWzZcrCzq6Js/v55VKpUGfr6+hLtEVHxxEaPiIiIiN5Ro4YjatRwzDUtOzsbDx7cx+3bsbkawNu3Y3H37h08fPgADx8+wNGjf+Z6nY6ODipUqAgHh2qoWrUaHByqoVq16qhSpSpMTEw0tk9ExQkbPSIiIiJSia6uLipVqoxKlSqjeXO3XPMyMzNx//5dZeN3+3YsYmNvIS7uNu7fv4e7d+/g7t072L9/r/I1MpkMFSpUgoODAxwcqqNqVQdlA2hqaqrp3VPKyclBWloaXr16hbS01LfP03J9/e/j32mpePXqFaysSsLFpT5cXOqjdOnSku0HFW9s9IiIiIgo3/T09N6etlnlnXmvX79GXNxt3Lx5HTduvHncvHkdsbG3cO/eHdy7dwcHDuzL9ZqKFSuhatU3DaCDw5ujgJ9/7vDRBjA9PR1JSUlITk5CUlLi269JymnJyclvnycqp6elKZCSolA2denp6fnOY8mSMABApUqVUa9eA2XjV61adejq6uZ7/UQfw0aPiIiIiNTKwMAA1apVR7Vq1XNNz8zMRFzcbdy4cS1XA3jr1t+4d+8u7t27iz/+2J/rNRUqVETVqg4wNTV727jlbuRev35dIDUbGxu/fZjAyMgIxsbGMDJ6d9q/z03eLmOE+/fv4fTpKJw9G608kvnPhW3MzMzh7FxX2fw5O9eFqalZgdRM9F9s9IiIiIhIEnp6eqha1QFVqzqgfft/p2dmZuLOnThcv37t7VHAa7hx4wZiY//G/fv3cP/+vQ+u08LCEhYWFrCwsIC5uQUsLCzffv3vNAuYm5vD3NwSlSt/htevoWzkjIyMIJPJ8r1/2dnZuHo1BqdPn0J0dBSio6Nw//49/PnnIfz55yEAbz6/+MUXNVCvXn3lUb8KFSoWyPapeJMJgiBIXYQYz5+nSF3CO2xszAplXYUdcxOHuYnD3MRjduIwN3GYmzjanltWVhbu3InDjRvXkZ7+CpaWlspG7p8GztDQ8JObJE3m9vjxI2XTd/r0KVy+fAlZWVm5lilT5jO4uNRHvXr14eRUF6VLl4G5uTnMzMwL1Wmf2j7e1KUgc7OxyftoMBu9AsTBLg5zE4e5icPcxGN24jA3cZibOMxNHClzS0tLw4UL55SNX3R0FBITE/Nc3szMPNdRydxHKP+Z9v4jmGZm5tDR0cGrV6/ee6GZfy4mk5qamutiNLmX/fe5oaE++vYdCDe31poLTAtoqtHjqZtERERERBIxNjZGo0auaNTIFcCbK37euvV3riN+SUmJys8gpqQkIyUlGcB9aQt/6+DBgxg0aCiCgydAT09P6nLoP9joEREREREVEjo6OsrPLXbv3jPXvOzsbCgUKbkuPvPfK4z+d9q78998BQBDQ8NcF5f59yIzeV9wxtj4vxecebPMtWsXMWnSJPz44w+IijqJpUtXonz5ClLERu/BRo+IiIiIqAjQ1dV9+3lES1Gvz87OVq6nIHTs2A41a7pgwIA+iI6OgpubK8LCfkLr1m0KZP2UPzpSF0BEREREROqnq6tb4BdzadCgIQ4ePIaWLVvj5cuX6NGjKyZNGofMzMwC3Q59OjZ6REREREQkWsmSJbFu3SZMnDgVurq6WLIkDB06eHzwNhikfmz0iIiIiIgoX3R0dDBoUCB++20vypUrj7Nno+Hm5oo9e3ZJXVqxxUaPiIiIiIgKRL169XHw4FG4u7dBYmIievXqhgkTgpGRkSF1acUOGz0iIiIiIiowVlYlsWbNRoSEzIBcLsfSpYvRoYM77t69I3VpxQobPSIiIiIiKlAymQwDBw5CZOReVKhQEefOnYWbWxPs2rVT6tKKDTZ6RERERESkFnXr1sPBg0fh4eGJ5OQkfPttd4wbNxqvX79W2zZfvnyBixfPF/vTRdnoERERERGR2lhalsDq1RswdepM6OnpYfnyn9CuXWvcuROX73Wnpqbi1KmT+OmnHzFgQB/Ur18bDg6V0apVU9Sq5YBJk8bh1q2/C2Avih7eMJ2IiIiIiNRKJpOhf/8AuLjUR79+3+LixfNwc2uCH35YjPbtv1ZpHZmZmbh2LQbnz5/DhQvncO7cWdy4cQ05OTm5ljM0NISNTSncv38PS5aEYcmSMDRo0Ag9evRC+/ZeMDIyUscuFjps9IiIiIiISCOcnOri4MGjCAwMwO7dO+Hv7wd//36YPHk6DAwMlMvl5OQgNvYWzp8/i/Pnz+LChXO4cuXyO6d86urq4ssvHVGnjhPq1HFG7dpOqFatOuRyOc6fP4t161Zj27YtOHXqBE6dOoFx48agU6cu6NGjN778soamd1+jZIIgCFIXIcbz5ylSl/AOGxuzQllXYcfcxGFu4jA38ZidOMxNHOYmDnMTh7mJk5/cBEHAzz//hMmTxyMzMxM1a9bGgAEBuHo1BhcunMPFixeQkpL8zuvs7augdm0n1KnjhNq1nVGjhiOMjY0/uC2FIgXbt2/FunWrcP78OeV0Jydn9OjRG15e3jA1NRW1H2IU5HizsTHLcx4bvQLE/yTEYW7iMDdxmJt4zE4c5iYOcxOHuYnD3MQpiNwuXDiHvn174969O+/MK1u23H+aOifUrl0HFhaW+drelSuXsW7dKmzZsgnJyUkAABMTU3Ts2Ak9evRC7dpOkMlk+drGx7DR+4jC+MPI/yTEYW7iMDdxmJt4zE4c5iYOcxOHuYnD3MQpqNySk5MwY8YU3L17B7Vq1VGeglm6dOkCqPL90tLSsHPnDqxbtxpRUSeV07/80hE9evRCp05d8t1U5oWN3kcUxh9G/ichDnMTh7mJw9zEY3biMDdxmJs4zE0c5iaOtuR28+YNrFu3Gps2bcCLFy8AvLmgS/v2XvDz64369RsW6FE+TTV6vL0CEREREREVW1WrOmDKlBm4ePEGli37BU2aNEN6ejo2b96IDh084Orqgm3bNktd5idjo0dERERERMWegYEBvLy8sXVrJKKiLiAwcARKlSqNv/++idGjh0td3ifj7RWIiIiIiIj+w9bWDuPGTcLo0WNx5MghtX1eT53Y6BEREREREb2Hnp4eWrZ0l7oMUTR66ubFixfh5+f3zvRDhw7B29sbXbt2xaZNmzRZEhERERERkdbR2BG95cuXIzIyEkZGRrmmZ2ZmYubMmdiyZQuMjIzQrVs3tGjRAtbW1poqjYiIiIiISKto7IhexYoVERYW9s702NhYVKxYERYWFtDX14ezszOio6M1VRYREREREZHW0dgRPXd3dzx48OCd6QqFAmZm/97/wcTEBAqF4qPrK1HCGHK5boHWWBA+dC8LyhtzE4e5icPcxGN24jA3cZibOMxNHOYmDnMTRxO5SX4xFlNTU6Smpiq/T01NzdX45eXlyzR1liWKttw0UtOYmzjMTRzmJh6zE4e5icPcxGFu4jA3cZibOMXmhun29va4e/cuEhMTkZGRgTNnzqBOnTpSl0VERERERFRkSXZEb+fOnUhLS0PXrl0RFBQEf39/CIIAb29vlC5dWqqyiIiIiIiIijyNNnrly5dX3j6hffv2yuktWrRAixYtNFkKERERERGR1pL81E0iIiIiIiIqWGz0iIiIiIiItIxMEARB6iKIiIiIiIio4PCIHhERERERkZZho0dERERERKRl2OgRERERERFpGTZ6REREREREWoaNHhERERERkZZho0dERERERKRl5FIXoA1ycnIwefJk3LhxA/r6+pg2bRoqVaokdVmFzsWLFzF37lysXbsWd+/eRVBQEGQyGT7//HNMmjQJOjo6+PHHH/Hnn39CLpdj7NixqFmzptRlSyYzMxNjx47Fw4cPkZGRgYEDB6JKlSrMTQXZ2dkYP3484uLiIJPJEBISAgMDA2angoSEBHTs2BErV66EXC5nZir65ptvYGpqCgAoX748unbtiunTp0NXVxeurq4YNGgQf1e8x9KlS3Ho0CFkZmaiW7duqFevHsfcR2zbtg3bt28HALx+/RrXrl3D2rVrOd4+IjMzE0FBQXj48CF0dHQwdepU/h+ngoyMDAQHB+P+/fswNTXFxIkTkZiYyPH2AWL/3s1r2XwRKN/27dsnjBkzRhAEQTh//rwwYMAAiSsqfJYtWya0a9dO6Ny5syAIgtC/f3/h1KlTgiAIwoQJE4T9+/cLV65cEfz8/IScnBzh4cOHQseOHaUsWXJbtmwRpk2bJgiCILx8+VJo2rQpc1PRgQMHhKCgIEEQBOHUqVPCgAEDmJ0KMjIyhO+//15o3bq1cOvWLWamovT0dOHrr7/ONa1Dhw7C3bt3hZycHKFv375CTEwMf1f8n1OnTgn9+/cXsrOzBYVCISxatIhj7hNNnjxZ2LhxI8ebCg4cOCAMGTJEEARBOHbsmDBo0CCONxWsXbtWGD9+vCAIghAbGyv06dOH4+0D8vP37vuWzS+eulkAzp49iyZNmgAAateujStXrkhcUeFTsWJFhIWFKb+PiYlBvXr1AABfffUVTpw4gbNnz8LV1RUymQxly5ZFdnY2Xrx4IVXJkvPw8EBgYCAAQBAE6OrqMjcVtWzZElOnTgUAPHr0CObm5sxOBaGhofDx8UGpUqUA8OdUVdevX8erV6/Qp08f9OzZE9HR0cjIyEDFihUhk8ng6uqqzI6/K/517NgxVK1aFQEBARgwYACaNWvGMfcJLl++jFu3bsHT05PjTQW2trbIzs5GTk4OFAoF5HI5x5sKbt26ha+++goAYGdnh8uXL3O8fUB+/t5937L5xUavACgUCuUpOwCgq6uLrKwsCSsqfNzd3SGX/3umsCAIkMlkAAATExOkpKS8k+M/04srExMTmJqaQqFQYMiQIRg6dChz+wRyuRxjxozB1KlT0b59e2b3Edu2bYOVlZXyFzXAn1NVGRoawt/fHytWrEBISAiCg4NhZGSknJ9XdsX9d8XLly9x5coVLFy4ECEhIRg5ciTH3CdYunQpAgIC8syH4y03Y2NjPHz4EG3atMGECRPg5+fH8aaC6tWr4/DhwxAEARcuXEBKSgqMjY2V8znecsvP37vvWza/+Bm9AmBqaorU1FTl9zk5Obn+keld/z3nODU1Febm5u/kmJqaCjMzMynKKzQeP36MgIAA+Pr6on379pgzZ45yHnP7uNDQUIwcORJdunTB69evldOZ3bu2bt0KmUyGkydP4tq1axgzZkyud7GZWd5sbW1RqVIlyGQy2NrawszMDImJicr5/2SXnp7O3xX/YWlpCTs7O+jr68POzg4GBgZ48uSJcj7HXN6Sk5MRFxeHBg0aQKFQvJMPx9u7Vq1aBVdXV4wYMQKPHz9Gr169kJmZqZzP8fZ+3t7eiI2Nha+vL5ycnFCtWjW8evVKOZ/j7cM+5e/d9y2b7+3new0EJycn/PXXXwCACxcuoGrVqhJXVPh98cUXiIqKAgD89ddfqFu3LpycnHDs2DHk5OTg0aNHyMnJgZWVlcSVSic+Ph59+vTBqFGj0KlTJwDMTVU7duzA0qVLAQBGRkaQyWSoUaMGs/uA9evXY926dVi7di2qV6+O0NBQfPXVV8xMBVu2bMGsWbMAAE+fPsWrV69gbGyMe/fuQRAEHDt2TJkdf1f8y9nZGUePHoUgCMrcGjZsyDGngujoaDRs2BDAmzeb9fT0ON4+wtzcXNmwWVhYICsri79TVXD58mU0bNgQv/76Kzw8PFC5cmWOt0/wKWPsfcvml0wQBCHfaynm/rnS0M2bNyEIAmbMmAF7e3upyyp0Hjx4gOHDh2PTpk2Ii4vDhAkTkJmZCTs7O0ybNg26uroICwvDX3/9hZycHAQHBxfIIC+qpk2bhj179sDOzk45bdy4cZg2bRpz+4i0tDQEBwcjPj4eWVlZ+O6772Bvb88xpyI/Pz9MnjwZOjo6zEwF/1yV7tGjR5DJZBg5ciR0dHQwY8YMZGdnw9XVFcOGDePviveYPXs2oqKiIAgChg0bhvLly3PMqeDnn3+GXC5H7969Abz5w5rj7cNSU1MxduxYPH/+HJmZmejZsydq1KjB8fYRL168wPDhw/Hq1SuYmZlh+vTpePz4McfbB4j9ezevZfODjR4REREREZGW4ambREREREREWoaNHhERERERkZZho0dERERERKRl2OgRERERERFpGTZ6REREREREWoaNHhERaRUHBwc4Ojoqv8/MzMTKlSvVvt3U1FSsW7dO+X1YWBgcHBywbNkytW+biIjo/7HRIyIirebl5YUFCxaodRuJiYnw9PTM1ejZ2dnBzc0NlStXVuu2iYiI3kcudQFERETqdOvWLejr66t1GwqFAo8fP4atra1ymqenJzw9PdW6XSIiorzwiB4REWmtFi1aAAAyMjLg4OCAqKgoAMDBgwfh6emJGjVqwNPTE/v371e+JigoCA4ODggLC4OrqytatWqFjIwMnD17Fl26dEHt2rXh5OSEPn364N69ewAANzc3AEBcXBwcHBzw4MGD9566uWfPHnh5ecHR0RHNmjXDjz/+iOzsbADAgwcP4ODggP79+2P16tVwdXWFs7MzZs6cCUEQNJIXERFpDzZ6RESktRo3bgwA0NHRgZubG0qUKIEbN25g8ODBePToEerVq4f4+HgEBgbi9OnTuV4bHh4OW1tb1KpVC5mZmRg4cCBiYmJQo0YNlCpVCsePH8eMGTNybcfY2Bhubm4wMjJ6p5bdu3dj6NChiI2NRZ06dZCeno6wsDBMnTo113JnzpxBeHg4KlasiNTUVKxatQonTpxQRzxERKTF2OgREZHW+qeJksvlCA8PR9WqVbFixQpkZ2cjLCwMK1euxMaNG5GTk4M1a9bkem379u2xdu1azJ07FxkZGRg8eDDmzJmDdevWYePGjQCAu3fvAgCmTJkCAChdujTCw8NRsmTJd2r54YcfAAA//fQT1qxZg99//x0WFhaIiIjAw4cPlculpaVhzZo12LBhA9q1awcAiImJKdhgiIhI67HRIyKiYuXWrVsAAH9/fzg4OMDDwwMAcOXKlVzL1a5dW/m8RIkSaN26NR4+fIj+/fujZcuWAIDXr1+rtM2XL1/i7t27sLa2Vh79++d5Tk4OLl26pFy2VKlScHBwAPDmgi7Am1NPiYiIPgUvxkJERMVKZmYmAMDV1RUGBgbK6YaGhrmWMzU1VT6/f/8+vvnmGxgaGsLPzw8DBgyAj4+PytvU0fnw+6oymey9dcjlb35N8zN6RET0qXhEj4iItJpMJsvVKFWpUgUA0LlzZ4SHh2P48OEoV64c2rZtm+t1urq6yuf79+9HSkoK2rdvj/79++eaB/zbyOXVkFlYWKBcuXKIj4/H8ePHAUD5XEdHB7Vq1cpVLxERUX6x0SMiIq1mZWWFzMxM+Pj4ICoqCt27d4dMJsPIkSPRq1cvdO/eHWvWrMHTp0/zXEfZsmUBAGvXrsW3336LXr16AXhzk3QAMDc3h66uLu7evQtfX1/cuXPnnXUMHDgQADBgwAD07NkT7dq1Q1JSEnx9ffHZZ58V8F4TEVFxx0aPiIi0WkBAACwtLREbG4v09HTUrVsXCxYsgK2tLc6ePQtjY2OMHDkS3bt3z3MdHh4e8PPzg5GREa5fv462bduiTp06SExMxJ07d2BqaorvvvsOpqamuHPnDtLT099ZR+fOnTF//nxUqVIF58+fh6GhIQIDAzFu3Dh17j4RERVTMoEn/hMREREREWkVHtEjIiIiIiLSMmz0iIiIiIiItAwbPSIiIiIiIi3DRo+IiIiIiEjLsNEjIiIiIiLSMmz0iIiIiIiItAwbPSIiIiIiIi3DRo+IiIiIiEjLsNEjIiIiIiLSMv8DE4MqJQWKz38AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use(\"seaborn\")\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(range(1,X.shape[1]+1,25),loss_list,\"k-\",linewidth=2);\n",
    "plt.xlabel(\"Iteration\",fontdict={\"size\":14,\"weight\":\"bold\"});\n",
    "plt.ylabel(\"Cost Function\",fontdict={\"size\":14,\"weight\":\"bold\"});\n",
    "plt.xticks(range(X.shape[1]+1)[::100]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction = model.predict(X)\n",
    "# prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc = 0\n",
    "# for i in range(1000):\n",
    "#     if np.array_equal(prediction[:,i],y[:,i]):\n",
    "#         acc += 1\n",
    "# acc/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_loss(prediction,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_pred = []\n",
    "\n",
    "# for i in range(X.shape[1]):\n",
    "#     _, _, _, _, a_output = forward(X[:,i])\n",
    "#     prediction = np.zeros(shape=a_output.shape[0])\n",
    "#     prediction[a_output.argmax()] = 1\n",
    "#     final_pred.append(list(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(final_pred).T[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_train(X, Y, model, train_flag=False):\n",
    "    prediction = model.predict(X)\n",
    "    acc = 0\n",
    "    for i in range(1000):\n",
    "        if np.array_equal(prediction[:,i],Y[:,i]):\n",
    "            acc += 1\n",
    "    print(f\"Accuracy of model : {acc*100/1000} %\")\n",
    "    \n",
    "    if train_flag:\n",
    "        loss_list = []\n",
    "        for i in range(X.shape[1]):\n",
    "            grad_hidden_layer, grad_output_layer = model.backward(X[:,i],Y[:,i])\n",
    "            model.internal_weights -= model.learning_rate * grad_hidden_layer\n",
    "            model.output_weights -= model.learning_rate * grad_output_layer\n",
    "            prediction = model.predict(X)\n",
    "            loss_list.append(compute_loss(prediction,Y))\n",
    "            if i%25 == 0:\n",
    "                print(f\"Inputs processed : {i}\")\n",
    "            \n",
    "        prediction = model.predict(X)\n",
    "        acc = 0\n",
    "        for i in range(1000):\n",
    "            if np.array_equal(prediction[:,i],Y[:,i]):\n",
    "                acc += 1\n",
    "        print(f\"Accuracy of model : {acc*100/1000} %\")\n",
    "        \n",
    "        plt.style.use(\"seaborn\")\n",
    "        plt.figure(figsize=(15,5))\n",
    "        plt.plot(range(1,X.shape[1]+1),loss_list,\"k-\",linewidth=2);\n",
    "        plt.xlabel(\"Iteration\",fontdict={\"size\":14,\"weight\":\"bold\"});\n",
    "        plt.ylabel(\"Cost Function\",fontdict={\"size\":14,\"weight\":\"bold\"});\n",
    "        plt.xticks(range(X.shape[1]+1)[::100]);\n",
    "        plt.savefig(\"./images/Cost_Function_Training.jpeg\",dpi=300)\n",
    "        print(\"Image saved at : ./images/Cost_Function_Training.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(input_size=X.shape[0]+1,\n",
    "                          hidden_size=150,\n",
    "                          num_classes=y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model : 18.2 %\n"
     ]
    }
   ],
   "source": [
    "batch_train(X, y, model, train_flag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_hidd_layer, grad_out_layer = model.backward(X[:,0],y[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 7)"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_out_layer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bias, z_internal, a_internal, _, a_output = model.forward(X[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7,)"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7,)"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_delta = np.subtract(a_output, y[:,0])\n",
    "cross_entropy_delta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def softmax(z: npt.ArrayLike) -> npt.ArrayLike:\n",
    "    \"\"\"\n",
    "    Softmax function.\n",
    "    \"\"\"\n",
    "    return np.divide(np.exp(z),np.sum(np.exp(z),axis=0))\n",
    "\n",
    "def relu(z: npt.ArrayLike) -> npt.ArrayLike:\n",
    "    \"\"\"\n",
    "    Rectified Linear Unit function.\n",
    "    \"\"\"\n",
    "    return np.array([np.max([0,i]) for i in z])\n",
    "\n",
    "# def softmax2(z: npt.ArrayLike) -> npt.ArrayLike:\n",
    "#     \"\"\"\n",
    "#     Softmax function.\n",
    "#     \"\"\"\n",
    "#     return np.exp(z.T) / np.sum(np.exp(z))\n",
    "\n",
    "def relu_prime(z: npt.ArrayLike) -> npt.ArrayLike:\n",
    "    \"\"\"\n",
    "    First derivative of ReLU function.\n",
    "    \"\"\"\n",
    "    return np.array([1 if i > 0 else 0 for i in z])\n",
    "\n",
    "\n",
    "# from typing import Tuple\n",
    "# import numpy as np\n",
    "# import numpy.typing as npt\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# from model.model_utils import softmax, relu, relu_prime\n",
    "\n",
    "\n",
    "def compute_loss(pred: npt.ArrayLike, truth: npt.ArrayLike) -> float:\n",
    "    \"\"\"\n",
    "    Compute the cross entropy loss.\n",
    "    \"\"\"\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(pred, epsilon, 1 - epsilon)\n",
    "    \n",
    "    return -np.mean(np.sum(truth * np.log(y_pred),axis=0)) / len(truth)\n",
    "\n",
    "\n",
    "class NeuralNetwork(object):\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_size: int,\n",
    "        hidden_size: int, \n",
    "        num_classes: int,\n",
    "        random_seed: int = 1,\n",
    "        learning_rate: float = 0.005,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize neural network's weights and biases.\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = num_classes\n",
    "        self.num_classes = num_classes\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        np.random.seed(random_seed)\n",
    "        self.internal_weights = np.random.uniform(low = -1.0,\n",
    "                                         high = 1.0,\n",
    "                                         size = (input_size,hidden_size),\n",
    "                                         )\n",
    "        np.random.seed(random_seed)\n",
    "        self.output_weights = np.random.uniform(low = -1,\n",
    "                                                high = 1,\n",
    "                                                size = (hidden_size,num_classes)\n",
    "                                                )\n",
    "        print(\"------------------Model Initialised------------------\")\n",
    "        \n",
    "    def forward(self,\n",
    "                X: npt.ArrayLike\n",
    "                )-> Tuple[npt.ArrayLike, npt.ArrayLike, npt.ArrayLike, npt.ArrayLike, npt.ArrayLike]:\n",
    "        \"\"\"\n",
    "        Forward pass with X as input matrix, returning the self prediction\n",
    "        Y_hat.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            bias = np.ones(shape=X.shape[1]).reshape(1,X.shape[1])\n",
    "            X = np.append(bias,X,axis=0)\n",
    "            # print(f\"Shape of Input Matrix : {X.shape}\")\n",
    "            z_internal: npt.ArrayLike = np.matmul(self.internal_weights.T,X)\n",
    "            # print(f\"Shape of internal weights : {z_internal.shape}\")\n",
    "            a_internal: npt.ArrayLike = np.array([relu(node) for node in z_internal])\n",
    "            # print(a_internal.shape)\n",
    "            z_output: npt.ArrayLike = np.matmul(self.output_weights.T,a_internal)\n",
    "            print(f\"Shape of output weights : {z_output.shape}\")\n",
    "            a_output: npt.ArrayLike = softmax(z_output)\n",
    "            # print(a_output.shape)\n",
    "            \n",
    "        except IndexError:\n",
    "            bias = [1]\n",
    "            X = np.append(bias,X,axis=0)\n",
    "            # print(f\"Shape of Input Matrix : {X.shape}\")\n",
    "            z_internal: npt.ArrayLike = np.matmul(self.internal_weights.T,X)\n",
    "            # print(f\"Shape of internal weights : {z_internal.shape}\")\n",
    "            a_internal: npt.ArrayLike = relu(z_internal)\n",
    "            # print(a_internal.shape)\n",
    "            z_output: npt.ArrayLike = np.matmul(self.output_weights.T,a_internal)\n",
    "            # print(f\"Shape of output weights : {z_output.shape}\")\n",
    "            a_output: npt.ArrayLike = softmax(z_output)\n",
    "            a_output = a_output.reshape(self.output_size,1)\n",
    "            # print(a_output.shape)\n",
    "        \n",
    "        return X, z_internal, a_internal, z_output, a_output\n",
    "\n",
    "    def predict(self, X: npt.ArrayLike) -> npt.ArrayLike:\n",
    "        \"\"\"\n",
    "        Create a prediction matrix with `self.forward()`\n",
    "        \"\"\"\n",
    "        try:\n",
    "            prediction = []\n",
    "            for i in range(X.shape[1]):\n",
    "                _, _, _, _, a_output = self.forward(X[:,i])\n",
    "                prediction_int = np.zeros(shape=self.output_size)\n",
    "                prediction_int[a_output.argmax()] = 1\n",
    "                prediction.append(list(prediction_int))\n",
    "            prediction = np.array(prediction).T\n",
    "        except IndexError:\n",
    "            _, _, _, _, a_output = self.forward(X)\n",
    "            prediction = np.zeros(shape=(self.output_size,1))\n",
    "            prediction[a_output.argmax()] = 1\n",
    "        \n",
    "        # print(prediction.shape)\n",
    "        return prediction\n",
    "\n",
    "    def backward(\n",
    "        self, \n",
    "        X: npt.ArrayLike, \n",
    "        Y: npt.ArrayLike\n",
    "    ) -> Tuple[npt.ArrayLike, npt.ArrayLike]:\n",
    "        \"\"\"\n",
    "        Backpropagation algorithm.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            batch_size = X.shape[1]\n",
    "            X_bias, z_internal, a_internal, _, a_output = self.forward(X)\n",
    "            \n",
    "            loss = compute_loss(a_output,Y)\n",
    "\n",
    "            cross_entropy_delta = np.subtract(a_output, Y)  # Derivative of softmax with cross-entropy loss\n",
    "            grad_output_layer = np.matmul(a_internal, cross_entropy_delta.T)\n",
    "            grad_hidden_input = np.matmul(self.output_weights,cross_entropy_delta) * np.array([relu_prime(z) for z in z_internal.T]).T\n",
    "            grad_hidden_layer = np.dot(X_bias,grad_hidden_input.T)\n",
    "        \n",
    "        except IndexError:\n",
    "            X_bias, z_internal, a_internal, _, a_output = self.forward(X)\n",
    "            Y = Y.reshape(self.output_size,1)\n",
    "            z_internal = z_internal.reshape(self.hidden_size,1)\n",
    "            a_internal = a_internal.reshape(self.hidden_size,1)\n",
    "            a_output = a_output.reshape(Y.shape)\n",
    "            # print(X_bias.shape)\n",
    "            # print(Y.shape)\n",
    "            # print(z_internal.shape)\n",
    "            # print(a_internal.shape)\n",
    "            # print(a_output.shape)\n",
    "            \n",
    "            loss = compute_loss(a_output,Y)\n",
    "            \n",
    "            cross_entropy_delta = np.subtract(a_output, Y)  # Derivative of softmax with cross-entropy loss\n",
    "            grad_output_layer = np.matmul(a_internal, cross_entropy_delta.T)\n",
    "            grad_hidden_input = np.matmul(self.output_weights,cross_entropy_delta) * relu_prime(z_internal).reshape(self.hidden_size,1)\n",
    "            grad_hidden_layer = np.dot(X_bias.reshape(self.input_size,1),grad_hidden_input.reshape(self.hidden_size,1).T)  \n",
    "            \n",
    "        return loss, grad_hidden_layer, grad_output_layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(287, 1000)\n",
      "(7, 1000)\n",
      "Input Matrix Shape: (287, 1000)\n",
      "Target Matrix Shape: (7, 1000)\n",
      "------------------Model Initialised------------------\n"
     ]
    }
   ],
   "source": [
    "sentences, intent, unique_intent = load_dataset(\"../data/dataset.csv\")\n",
    "\n",
    "X = bag_of_words_matrix(sentences=sentences,COUNT_THRESHOLD=2)\n",
    "Y = labels_matrix(data=(intent,unique_intent))\n",
    "print(f\"Input Matrix Shape: {X.shape}\")\n",
    "print(f\"Target Matrix Shape: {Y.shape}\")\n",
    "\n",
    "model = NeuralNetwork(input_size=X.shape[0]+1,\n",
    "                        hidden_size=150,\n",
    "                        num_classes=Y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE = 64\n",
    "# START_INDEX = 0\n",
    "# NUM_EXAMPLES = 1000\n",
    "\n",
    "# i = 0\n",
    "# while i < NUM_EXAMPLES:\n",
    "#     print(X[:,i:i+BATCH_SIZE].shape)\n",
    "#     i += BATCH_SIZE\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_train(X, Y, model, train_flag=False):\n",
    "    NUM_EXAMPLES = X.shape[1]\n",
    "    prediction = model.predict(X)\n",
    "    hits = np.sum([np.array_equal(prediction[:,i],Y[:,i]) for i in range(X.shape[1])])\n",
    "    print(f\"Accuracy of model : {hits*100/X.shape[1]} %\")\n",
    "    \n",
    "    if train_flag:\n",
    "        loss_list = []\n",
    "        accuracy_list = []\n",
    "        for i in range(NUM_EXAMPLES):\n",
    "            loss, grad_hidden_layer, grad_output_layer = model.backward(X[:,i],Y[:,i])\n",
    "            model.internal_weights -= model.learning_rate * grad_hidden_layer\n",
    "            model.output_weights -= model.learning_rate * grad_output_layer\n",
    "            # loss_list.append(loss)\n",
    "            if i%25 == 0:\n",
    "                print(f\"Inputs processed : {i}\")\n",
    "                prediction = model.predict(X)\n",
    "                hits = np.sum([np.array_equal(prediction[:,i],Y[:,i]) for i in range(NUM_EXAMPLES)])\n",
    "                loss_list.append(compute_loss(prediction,Y))\n",
    "                accuracy_list.append(hits*100/1000)\n",
    "                print(f\"Current loss : {compute_loss(prediction,Y)} \\tCurrent Accuracy : {hits*100/NUM_EXAMPLES} %\")\n",
    "                print(f\"Batch loss : {loss}\")\n",
    "        prediction = model.predict(X)\n",
    "        hits = np.sum([np.array_equal(prediction[:,i],Y[:,i]) for i in range(NUM_EXAMPLES)])\n",
    "        print(f\"Accuracy of model : {hits*100/NUM_EXAMPLES} %\")\n",
    "        \n",
    "def minibatch_train(X, Y, model, BATCH_SIZE:int = 64, train_flag=False):\n",
    "    NUM_EXAMPLES = X.shape[1]\n",
    "    prediction = model.predict(X)\n",
    "    hits = np.sum([np.array_equal(prediction[:,i],Y[:,i]) for i in range(NUM_EXAMPLES)])\n",
    "    print(f\"Accuracy of model : {hits*100/NUM_EXAMPLES} %\")\n",
    "    \n",
    "    if train_flag:\n",
    "        loss_list = []\n",
    "        accuracy_list = []\n",
    "        \n",
    "        i = 0\n",
    "        while i < NUM_EXAMPLES:\n",
    "            loss, grad_hidden_layer, grad_output_layer = model.backward(X[:,i:i+BATCH_SIZE],Y[:,i:i+BATCH_SIZE])\n",
    "            model.internal_weights -= model.learning_rate * grad_hidden_layer\n",
    "            model.output_weights -= model.learning_rate * grad_output_layer\n",
    "            print(f\"loss : {loss}\")\n",
    "            print(f\"Inputs processed : {i}\")\n",
    "            prediction = model.predict(X)\n",
    "            hits = np.sum([np.array_equal(prediction[:,i],Y[:,i]) for i in range(NUM_EXAMPLES)])\n",
    "            loss_list.append(compute_loss(prediction,Y))\n",
    "            accuracy_list.append(hits*100/1000)\n",
    "            print(f\"Current loss : {compute_loss(prediction,Y)} \\tCurrent Accuracy : {hits*100/NUM_EXAMPLES} %\")\n",
    "            i += BATCH_SIZE\n",
    "            \n",
    "    return loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_train(X,Y,model,train_flag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model : 19.4 %\n",
      "Shape of output weights : (7, 64)\n",
      "loss : 1.5774910176822172\n",
      "Inputs processed : 0\n",
      "Current loss : 3.8880793998842305 \tCurrent Accuracy : 21.2 %\n",
      "Shape of output weights : (7, 64)\n",
      "loss : 1.7857261336524135\n",
      "Inputs processed : 64\n",
      "Current loss : 4.006498061809638 \tCurrent Accuracy : 18.8 %\n",
      "Shape of output weights : (7, 64)\n",
      "loss : 1.694996705298218\n",
      "Inputs processed : 128\n",
      "Current loss : 3.157830984677548 \tCurrent Accuracy : 36.0 %\n",
      "Shape of output weights : (7, 64)\n",
      "loss : 1.2432124832765565\n",
      "Inputs processed : 192\n",
      "Current loss : 3.281183757516515 \tCurrent Accuracy : 33.5 %\n",
      "Shape of output weights : (7, 64)\n",
      "loss : 1.0364961754603839\n",
      "Inputs processed : 256\n",
      "Current loss : 3.2811837575165135 \tCurrent Accuracy : 33.5 %\n",
      "Shape of output weights : (7, 64)\n",
      "loss : 0.9040346063285349\n",
      "Inputs processed : 320\n",
      "Current loss : 2.4966601222606863 \tCurrent Accuracy : 49.4 %\n",
      "Shape of output weights : (7, 64)\n",
      "loss : 0.8408179282835656\n",
      "Inputs processed : 384\n",
      "Current loss : 2.323966240286133 \tCurrent Accuracy : 52.9 %\n",
      "Shape of output weights : (7, 64)\n",
      "loss : 0.4803177731074099\n",
      "Inputs processed : 448\n",
      "Current loss : 1.8009504834489143 \tCurrent Accuracy : 63.5 %\n",
      "Shape of output weights : (7, 64)\n",
      "loss : 0.2613055818780528\n",
      "Inputs processed : 512\n",
      "Current loss : 1.5591790486845394 \tCurrent Accuracy : 68.4 %\n",
      "Shape of output weights : (7, 64)\n",
      "loss : 0.2836410282216199\n",
      "Inputs processed : 576\n",
      "Current loss : 1.549310826857422 \tCurrent Accuracy : 68.6 %\n",
      "Shape of output weights : (7, 64)\n",
      "loss : 0.3304773551860863\n",
      "Inputs processed : 640\n",
      "Current loss : 1.5295743832031874 \tCurrent Accuracy : 69.0 %\n",
      "Shape of output weights : (7, 64)\n",
      "loss : 0.4781572153901864\n",
      "Inputs processed : 704\n",
      "Current loss : 1.4901014958947183 \tCurrent Accuracy : 69.8 %\n",
      "Shape of output weights : (7, 64)\n",
      "loss : 0.2638760081952673\n",
      "Inputs processed : 768\n",
      "Current loss : 1.2483300611303432 \tCurrent Accuracy : 74.7 %\n",
      "Shape of output weights : (7, 64)\n",
      "loss : 0.2393251000599658\n",
      "Inputs processed : 832\n",
      "Current loss : 1.2483300611303434 \tCurrent Accuracy : 74.7 %\n",
      "Shape of output weights : (7, 64)\n",
      "loss : 0.19439367522336312\n",
      "Inputs processed : 896\n",
      "Current loss : 1.1397796210320525 \tCurrent Accuracy : 76.9 %\n",
      "Shape of output weights : (7, 40)\n",
      "loss : 0.30625584989337234\n",
      "Inputs processed : 960\n",
      "Current loss : 1.0855044009829071 \tCurrent Accuracy : 78.0 %\n"
     ]
    }
   ],
   "source": [
    "ll = minibatch_train(X,Y,model,BATCH_SIZE=64,train_flag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1000 // 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(range(1,1000+1,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anlp2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
